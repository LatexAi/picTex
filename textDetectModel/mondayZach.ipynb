{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PicTex Text Detection Model with Zach\n",
    "\n",
    "Finalized on **August 21st, 2020**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 73 classes\n",
      "{'(': 0, ')': 1, '+': 2, '-': 3, '0': 4, '1': 5, '2': 6, '3': 7, '4': 8, '5': 9, '6': 10, '7': 11, '8': 12, '9': 13, '=': 14, 'a': 15, 'alpha': 16, 'ast': 17, 'b': 18, 'beta': 19, 'c': 20, 'comma': 21, 'd': 22, 'delta': 23, 'e': 24, 'emptyset': 25, 'f': 26, 'forall': 27, 'full_stop': 28, 'g': 29, 'greater': 30, 'h': 31, 'implies': 32, 'in': 33, 'infty': 34, 'int': 35, 'j': 36, 'k': 37, 'l': 38, 'lambda': 39, 'land': 40, 'leq': 41, 'lesser': 42, 'm': 43, 'mu': 44, 'n': 45, 'nabla': 46, 'Naturals': 47, 'neq': 48, 'o': 49, 'p': 50, 'perp': 51, 'pi': 52, 'q': 53, 'r': 54, 'Reals': 55, 's': 56, 'setminus': 57, 'sigma': 58, 'sim': 59, 'sum': 60, 'supset': 61, 't': 62, 'theta': 63, 'u': 64, 'v': 65, 'varepsilon': 66, 'w': 67, 'x': 68, 'y': 69, 'z': 70, '[': 71, ']': 72}\n",
      "{0: '(', 1: ')', 2: '+', 3: '-', 4: '0', 5: '1', 6: '2', 7: '3', 8: '4', 9: '5', 10: '6', 11: '7', 12: '8', 13: '9', 14: '=', 15: 'a', 16: 'alpha', 17: 'ast', 18: 'b', 19: 'beta', 20: 'c', 21: 'comma', 22: 'd', 23: 'delta', 24: 'e', 25: 'emptyset', 26: 'f', 27: 'forall', 28: 'full_stop', 29: 'g', 30: 'greater', 31: 'h', 32: 'implies', 33: 'in', 34: 'infty', 35: 'int', 36: 'j', 37: 'k', 38: 'l', 39: 'lambda', 40: 'land', 41: 'leq', 42: 'lesser', 43: 'm', 44: 'mu', 45: 'n', 46: 'nabla', 47: 'Naturals', 48: 'neq', 49: 'o', 50: 'p', 51: 'perp', 52: 'pi', 53: 'q', 54: 'r', 55: 'Reals', 56: 's', 57: 'setminus', 58: 'sigma', 59: 'sim', 60: 'sum', 61: 'supset', 62: 't', 63: 'theta', 64: 'u', 65: 'v', 66: 'varepsilon', 67: 'w', 68: 'x', 69: 'y', 70: 'z', 71: '[', 72: ']'}\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "data_dir = \"./final/\"\n",
    "\n",
    "classes = os.listdir(data_dir)\n",
    "num_classes = len(classes)\n",
    "\n",
    "classes_encode, classes_decode = {}, {}\n",
    "for i, name in enumerate(classes):\n",
    "    classes_encode[name] = i\n",
    "    classes_decode[i] = name\n",
    "\n",
    "encode_dict, decode_dict = OrderedDict(classes_encode), OrderedDict(classes_encode)\n",
    "\n",
    "print(f\"There are {num_classes} classes\")\n",
    "print(classes_encode)\n",
    "print(classes_decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create the `Dataset` and `Dataloader` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# grayscale images have one channel\n",
    "num_output_channels = 1\n",
    "normal = (0.5,)\n",
    "\n",
    "# choose batch size\n",
    "batch_size = 64\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Grayscale(num_output_channels),\n",
    "     transforms.Resize((32, 32)), \n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(normal, normal)])\n",
    "\n",
    "class PicTexDataset(Dataset):\n",
    "    def __init__(self, root_dir, encode_dict, num_classes, transform=None, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory containing images sorted by class folders\n",
    "            encode_dict (Ordered dict): Dictionary with class names zipped from 0-(num_classes-1)\n",
    "            num_classes (int): Number of classes (SHOULD EQUAL LEN of ENCODE_DICT)\n",
    "            transform (torchvision.transforms): Transforms to be applied to images \n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.encode_dict = encode_dict\n",
    "        self.num_classes = num_classes\n",
    "        self.transform = transform\n",
    "        self.is_train = train\n",
    "        \n",
    "        \"\"\"\n",
    "        Loading images:\n",
    "            all_paths (string list): Path of every image\n",
    "            all_paths_class (string list): Bijection with all_paths. Class name for each path\n",
    "            all_images (string * string list): List of these objects... (class name, path of image)\n",
    "        \"\"\"\n",
    "        all_paths, all_paths_class = [], []\n",
    "        for name in encode_dict.keys():\n",
    "            list_classes = os.listdir(root_dir + name)\n",
    "            all_paths += list(map(lambda s : root_dir + name + \"/\" + s, list_classes))\n",
    "            all_paths_class += [name] * len(list_classes)\n",
    "        \n",
    "        self.all_images = list(zip(all_paths_class, all_paths))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_class, img_name = self.all_images[idx]\n",
    "        try:\n",
    "            image = Image.open(img_name)\n",
    "            if self.is_train:\n",
    "                image = self.random_shift(image)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except OSError:\n",
    "            print(img_name, img_class)\n",
    "        \n",
    "        \"\"\"\n",
    "        label:\n",
    "            If we wanted a tensor with a 1 in the spot of the class\n",
    "            just uncomment the old label definition\n",
    "            but NLLoss and CrossEntropyLoss just want the index\n",
    "        \"\"\"\n",
    "        #label = torch.zeros(self.num_classes)\n",
    "        #label[self.encode_dict[img_class]] = 1 \n",
    "        label = self.encode_dict[img_class]\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def random_shift(self, img):\n",
    "        if random.random() < 0.5:\n",
    "            return img\n",
    "\n",
    "        img = np.array(img)\n",
    "        h, w = img.shape\n",
    "        img_out = np.zeros((h, w), dtype=np.double)\n",
    "\n",
    "        dx = random.uniform(-w*0.15, w*0.15)\n",
    "        dy = random.uniform(-h*0.15, h*0.15)\n",
    "        dx, dy = int(dx), int(dy)\n",
    "\n",
    "        if dx >= 0 and dy >= 0:\n",
    "            img_out[dy:, dx:] = img[:h-dy, :w-dx]\n",
    "        elif dx >= 0 and dy < 0:\n",
    "            img_out[:h+dy, dx:] = img[-dy:, :w-dx]\n",
    "        elif dx < 0 and dy >= 0:\n",
    "            img_out[dy:, :w+dx] = img[:h-dy, -dx:]\n",
    "        elif dx < 0 and dy < 0:\n",
    "            img_out[:h+dy, :w+dx] = img[-dy:, -dx:]\n",
    "\n",
    "        img_out = Image.fromarray(img_out) \n",
    "        return img_out\n",
    "        \n",
    "    \n",
    "def load_split_train_test(datadir, valid_size = .2):\n",
    "    dataset = PicTexDataset(data_dir, encode_dict, num_classes, transform)\n",
    "    num_test = int(valid_size*len(dataset))\n",
    "    num_train = len(dataset) - num_test\n",
    "    \n",
    "    train_data, test_data = random_split(dataset, (num_train, num_test))\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    testloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    return trainloader, testloader\n",
    "\n",
    "trainloader, testloader = load_split_train_test(data_dir, .15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test `Dataloader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: tensor([68, 36,  5, 20, 10, 58, 58, 57, 22, 66, 15, 11, 10, 50, 30, 57,  6, 32,\n",
      "        49, 10, 61,  6,  2, 11, 49, 20, 36, 52, 29, 71, 63, 38, 69,  2, 64, 19,\n",
      "        32, 20, 71, 19, 48, 49, 58, 12, 43, 66, 44, 22, 51, 26, 16, 40, 63, 45,\n",
      "        23, 54, 30, 42, 64, 32, 15, 43, 10, 62])\n",
      "Label: tensor([50, 47, 50, 38, 40,  4, 67, 22, 72, 52, 48, 30,  1, 67, 46, 24, 58, 57,\n",
      "        49, 39, 43, 19, 57, 20, 68,  6, 67, 46,  5, 31, 17, 68, 58, 19, 59, 44,\n",
      "        66, 61, 48, 30, 44,  5, 32, 41, 27,  6, 25, 60, 11, 47, 57, 18, 43, 60,\n",
      "        25, 12, 45, 69, 69, 55, 17, 29, 57, 40])\n",
      "Label: tensor([ 3, 37, 27, 14, 17,  6, 68, 71, 38, 14, 54,  9, 38, 53, 53, 56,  8, 67,\n",
      "        56, 25, 52,  4,  2, 14, 66, 34, 44, 26, 45, 19, 66, 45, 16, 67, 30, 40,\n",
      "        31, 39, 66, 11, 70, 60, 45, 24, 15, 47, 29, 40, 11, 12,  8, 46, 63, 55,\n",
      "        62, 25, 66, 70, 37, 42, 36, 68, 27, 59])\n",
      "Label: tensor([44,  0, 57, 70, 34, 53,  6, 44,  2, 11, 68, 62, 35, 30, 23, 21,  4, 64,\n",
      "         7, 63, 24, 23, 18,  9, 58, 17, 32, 55, 45, 32,  6, 10, 53, 63, 19, 59,\n",
      "        37, 58,  7, 44, 65,  8, 40, 16, 54, 72,  1, 54, 21, 55, 55, 59,  9, 70,\n",
      "        62, 46, 11, 34,  3, 68, 52,  0, 30, 29])\n",
      "Label: tensor([46, 37, 34, 13, 41, 34, 58, 21, 63, 18,  8, 47, 19, 48, 54,  3, 36, 40,\n",
      "        66,  9, 46, 52, 57, 41, 31, 48, 67, 11, 30, 11, 60, 52, 42, 59, 54, 36,\n",
      "        46, 44, 23, 65, 58, 46, 33, 58, 70, 19,  9,  1,  6, 68, 70,  8, 62, 46,\n",
      "         9, 60, 59, 57,  3, 57,  9, 32, 11, 69])\n",
      "Label: tensor([67, 40, 65,  5, 17,  7, 19, 57,  0, 65, 25,  6, 70, 18, 36, 72, 17, 40,\n",
      "        33, 51, 25, 23, 70, 50, 54, 40, 15, 53, 31, 14,  7, 14, 17, 63, 38, 23,\n",
      "        70, 43, 42,  6,  1, 17,  1, 48, 55, 13, 27, 13, 45,  6, 56, 53, 50, 38,\n",
      "        31,  9,  7, 54, 30,  9, 31, 43, 63, 14])\n",
      "Label: tensor([68, 36, 51, 32, 12, 30, 39, 58, 22, 32, 30,  7, 58, 23, 33, 21, 63,  9,\n",
      "        48, 45, 45, 18,  2, 38, 17, 16, 29,  4, 26, 52, 14, 37, 58, 60, 26,  5,\n",
      "        12,  6, 21, 64, 40, 56, 70,  3, 52, 26, 59, 69, 18, 67, 62, 12, 70, 33,\n",
      "        27,  3, 32, 67, 17, 14, 64, 63, 32, 12])\n",
      "Label: tensor([11,  2, 31, 59, 13, 51, 39, 29, 25, 17,  6, 18, 30, 23, 67, 65, 33,  5,\n",
      "        53, 67, 20, 31,  2, 44, 60, 54, 55, 22, 24, 52, 21, 53, 39, 11, 49, 10,\n",
      "         9, 10, 64, 24, 58, 66, 13,  8, 15, 60, 37, 42, 11,  5, 11, 46,  5, 38,\n",
      "        14, 10, 40, 30, 56, 64, 51, 67, 42,  3])\n",
      "Label: tensor([51, 19, 59, 26, 26,  6, 45, 67, 58,  1, 42, 60, 45, 57, 32, 14, 26, 49,\n",
      "        46, 47, 68, 30, 24, 19, 15, 57, 61, 39, 71, 32,  4, 17, 18, 54, 38,  7,\n",
      "        36, 71, 37, 32, 19, 20, 68,  3,  8, 24, 37, 25, 30,  4, 71, 30,  1, 48,\n",
      "        21,  0, 32, 70, 61,  6, 65, 26, 16, 13])\n",
      "Label: tensor([72, 35,  9, 57,  7, 50,  9, 23, 15, 11, 47, 50, 23, 59, 55, 34, 49, 56,\n",
      "        34, 69, 20,  9, 31,  3, 21, 33, 57, 45, 11,  5, 38, 70, 45, 42, 71, 65,\n",
      "        21, 50,  7, 46,  3, 15, 54, 42, 32, 65, 56, 65, 17, 56, 70, 44, 48, 16,\n",
      "        40, 72,  1, 32, 30, 68, 51, 14,  9, 49])\n",
      "Label: tensor([38,  0,  5, 49, 69, 61,  0, 52, 13, 43, 63, 55, 17, 44, 66, 21, 35, 26,\n",
      "        70, 22, 60, 46, 24, 16, 42, 12,  9, 27, 13, 24, 66, 36, 17, 63, 32, 47,\n",
      "         4, 37, 53, 47, 21, 68, 29, 27, 65, 59, 37, 71,  0, 55, 24, 72, 24, 39,\n",
      "        51,  7, 54, 45, 24, 34, 17, 70, 41, 57])\n",
      "Label: tensor([26, 42, 11, 38, 61, 33, 55, 29, 10, 12, 30, 71, 31, 31,  6, 20, 44, 63,\n",
      "        27, 57, 11, 48,  8, 21, 16, 12, 58, 45,  4, 64, 31, 65, 46,  6, 31, 31,\n",
      "        38, 50, 30, 15,  0, 44, 23, 36, 44, 72, 42, 26, 31, 58, 22,  2, 55, 39,\n",
      "        59, 18, 67, 45, 54, 53, 65, 60, 15, 62])\n",
      "Label: tensor([10,  8, 20, 27, 24, 17,  5,  5, 67,  7, 30, 45, 22, 64, 42, 23, 55, 28,\n",
      "        54, 63, 52, 33, 63, 55, 67, 36, 52, 26,  9, 43, 10,  0,  9, 24, 10, 30,\n",
      "        22, 67, 48, 54, 20, 61,  2,  5, 45, 33, 11, 12,  3, 60, 39,  8, 12, 11,\n",
      "        38, 17, 65, 40, 38, 37, 19, 10, 50, 29])\n",
      "Label: tensor([19, 65, 72, 68, 62, 10, 13, 25, 40, 18, 60, 56, 35, 65, 27, 57, 54,  6,\n",
      "        58, 48,  0, 52, 36, 33, 70, 62, 34, 42,  1, 24, 72, 23, 26, 27, 59,  5,\n",
      "        62, 49,  8, 15, 72, 59, 10, 56, 68, 40, 50, 34, 12, 54, 72, 17,  5, 17,\n",
      "        49,  4, 11, 17, 55, 22, 11, 42, 30, 38])\n",
      "Label: tensor([ 6, 22, 66, 29, 15, 21, 67, 26, 51,  8, 23, 13, 12, 40, 24, 19, 32, 46,\n",
      "        38, 27, 70, 19, 48, 21, 23,  6, 43,  5,  1, 15, 26, 12, 21, 44, 54, 23,\n",
      "        16, 14, 34,  8, 60, 27, 63, 22, 50, 49, 70, 24,  6, 67, 36,  4, 22, 32,\n",
      "        39, 54, 55, 72, 70, 72, 51,  5, 60, 26])\n",
      "Label: tensor([18, 53, 45, 49, 16, 10, 21, 39, 72, 69, 34, 35, 61, 11, 30, 57,  1, 71,\n",
      "        62, 56, 16, 70, 64, 14, 23,  4, 72, 34, 57, 29, 17, 34, 26, 53, 42, 58,\n",
      "        68, 72, 24, 68, 67, 66, 31, 45, 44,  5, 67, 61, 53, 26, 47, 60,  0, 41,\n",
      "        61, 63, 69, 72, 51, 29, 65, 67, 15, 30])\n",
      "Label: tensor([40, 28, 57, 63,  6, 11, 31, 27,  5, 25, 16, 17, 49, 45,  0, 60, 11, 15,\n",
      "        29, 16, 65, 56, 21,  5, 42, 47,  2, 58, 22, 66, 15, 20, 64, 37, 38, 59,\n",
      "        55, 67,  9,  1, 38, 29, 65,  0, 45, 15, 10, 40, 23, 10, 37, 67, 12, 24,\n",
      "        59,  6, 36, 34,  1, 44,  5, 65, 54, 62])\n",
      "Label: tensor([58, 18, 42, 40, 26, 10, 61, 45, 27, 63, 72, 22, 69, 56, 22, 51, 34, 56,\n",
      "        14, 45, 39,  9, 17, 50, 68, 52, 38, 71,  4, 31, 54, 50, 58,  1, 19, 28,\n",
      "        58, 50, 26, 12, 30, 69, 32, 66, 27, 64, 63, 45, 72, 38, 46, 52,  6, 40,\n",
      "        26, 59,  4, 26, 60, 14, 33, 52, 65, 64])\n",
      "Label: tensor([ 3, 57, 47, 72, 66, 19, 39, 15, 24, 50, 23, 59, 70, 26, 24, 14,  9, 45,\n",
      "        36, 57,  7, 38, 14, 33, 19, 16, 23, 51, 24, 32, 10, 49, 61, 71, 24, 37,\n",
      "        17, 59, 49, 64, 58, 70, 46, 31, 52, 22, 22, 57, 62, 68,  6,  7, 60, 63,\n",
      "        26, 32, 35, 29, 69, 36, 32, 56, 37, 67])\n",
      "Label: tensor([24, 45, 58, 33, 61, 45, 24,  8, 52, 49, 67, 59, 40, 29, 61, 49, 45,  6,\n",
      "        57, 23, 27, 69, 10, 35, 38,  6, 53, 47, 65, 70, 64, 12, 37, 26, 43, 29,\n",
      "        62, 26,  2,  4, 11, 60, 38, 71, 47, 16, 53, 50,  9, 62, 22, 16, 61,  4,\n",
      "        36, 47, 47,  8,  3, 12, 20,  7, 58, 21])\n",
      "Label: tensor([36, 61, 43, 20, 39, 18, 66, 27, 14, 55, 31, 14, 13, 45, 31, 56, 20, 22,\n",
      "         9, 11, 64, 45, 36, 29, 40, 54, 38, 16, 72, 20, 12, 43, 21, 52, 30, 37,\n",
      "        19, 60, 43, 46, 49, 57,  3, 63, 45, 62,  6, 36, 69, 63, 63, 57, 30, 66,\n",
      "        47, 24, 14, 62, 67, 35, 35, 71, 22, 14])\n",
      "Label: tensor([70, 65, 45, 29, 34, 10, 61, 47,  8,  9, 65, 36, 23, 31, 32, 37,  4, 47,\n",
      "        45, 11,  4, 57, 45, 53,  5, 55, 19,  3, 37, 45, 37, 70, 61, 68, 39,  9,\n",
      "        38, 40, 66,  9, 12, 19, 24, 55, 17, 63,  1, 72, 53, 20, 30,  2, 36, 11,\n",
      "        10, 62, 53, 31, 56, 17, 18, 14, 49, 68])\n",
      "Label: tensor([31, 49, 32, 71, 38, 61, 34,  5, 68, 51, 64, 57, 37, 63, 70, 59,  5, 71,\n",
      "         4,  7, 11, 12,  3, 53, 33, 32, 49, 18, 35, 35,  9, 11, 40, 64, 67, 57,\n",
      "        18, 63, 36, 63, 12,  7,  2, 24, 32, 60, 47, 37, 54, 14, 17, 58,  3, 11,\n",
      "        14, 66, 24, 66,  6, 68, 57, 58, 70,  7])\n",
      "Label: tensor([ 1, 54, 50, 26, 12, 30, 30, 55, 44, 27, 16, 30, 59, 62, 69, 48, 70, 47,\n",
      "        29,  3, 30, 51, 43, 14,  6, 50, 14, 65, 26, 43, 10, 23, 46, 27, 19, 71,\n",
      "        19, 24, 66, 14,  8,  7, 25, 42, 66,  4, 20, 47, 65, 25, 27,  1, 44, 52,\n",
      "        52, 21, 20, 17, 49, 70, 14, 52, 17, 24])\n",
      "Label: tensor([30, 22, 58, 29, 39, 10, 32, 49, 48, 20, 63, 57, 45, 34, 54, 10, 48, 31,\n",
      "        68, 60, 27, 11, 22, 10, 58, 44,  9, 62,  0, 55, 32, 10, 32, 64, 44, 68,\n",
      "         6,  4, 30, 32, 17, 32, 53, 45, 15, 65, 69, 66, 45,  3, 63, 34, 37, 21,\n",
      "        49, 20, 28, 65, 53, 56, 44, 67, 40, 60])\n",
      "Label: tensor([44, 44,  4, 42, 33, 24,  0, 71,  3, 31, 37, 65, 42, 57,  2, 30,  7, 62,\n",
      "        12, 43, 23,  6, 27, 17, 11, 38, 10,  4, 25, 43, 56, 34, 71, 26, 47, 22,\n",
      "         9, 70, 54, 48, 70,  8, 49,  0, 72, 10, 45, 40, 13, 65,  3, 66, 36,  6,\n",
      "        52, 26, 25, 16, 40,  0, 34, 55, 39, 46])\n",
      "Label: tensor([64, 58, 68, 69, 14, 57, 20,  0,  0, 13, 44, 29,  2, 24, 72, 63, 31,  7,\n",
      "         7, 34, 49, 63, 27,  5, 70, 12, 59,  7, 71,  1, 67, 37, 19, 71, 64, 34,\n",
      "        17, 66, 18,  0, 17, 33, 23, 25, 49, 51, 31, 10, 62, 72, 58, 44, 33,  0,\n",
      "        34, 61, 33, 27, 38, 14, 62, 70, 22, 32])\n",
      "Label: tensor([15, 54,  5,  9, 15, 60,  2, 58, 63, 25, 51, 22, 24, 61, 52, 60, 70, 24,\n",
      "        49, 31,  4, 19, 49, 53,  4, 55, 60, 65, 30, 69, 36, 19, 58, 14, 59, 61,\n",
      "        64,  3,  4, 38, 32,  5, 54, 44, 29, 40, 39, 30, 36, 52,  8, 44,  4, 66,\n",
      "        33,  2, 29,  7, 18, 15,  6,  8, 23, 65])\n",
      "Label: tensor([24, 12, 10, 37, 43, 26, 12, 46, 50, 59,  6, 21, 51, 20, 65, 12, 66, 61,\n",
      "        37, 46, 34, 38, 14,  3, 55, 58, 65, 52, 12,  2, 45, 59, 24, 44, 30, 68,\n",
      "        22, 39, 15, 58, 34, 12, 15, 29, 43, 10, 39, 20, 11, 34, 56, 34, 26, 31,\n",
      "        21, 10, 58, 51, 21, 66, 24, 56, 59, 47])\n",
      "Label: tensor([ 7, 71, 30, 61,  2, 38, 47, 58, 18, 23,  5,  9,  9, 69, 70,  8, 30, 67,\n",
      "        26,  7, 38, 20, 44, 70, 30, 34, 10, 62,  4, 47, 64, 68, 10,  3, 12, 43,\n",
      "        50, 42, 59,  5, 26, 70, 60, 17, 55, 44, 32, 13, 31,  5, 39, 37, 20, 50,\n",
      "        32, 53, 43,  6, 31, 69,  1, 53, 29, 23])\n",
      "Label: tensor([14, 42, 25, 24, 72, 14, 25, 52, 32, 50, 69, 29, 34, 44, 48, 71, 38, 43,\n",
      "        47,  6, 26, 65, 68,  9, 18, 49, 34, 15, 56, 72, 61,  8, 33, 33, 33, 68,\n",
      "        17, 53, 54, 45, 70,  6,  2, 12, 37, 56, 58,  9, 13, 43, 52, 66,  0, 24,\n",
      "        51,  2, 13,  9, 43, 25, 37, 48, 65, 22])\n",
      "Label: tensor([46,  6, 59, 58, 25,  2,  5, 61, 72, 12, 55, 45, 64, 51, 54, 12, 33, 62,\n",
      "        22, 22, 28, 39, 52, 27, 40, 11,  2,  3, 53, 27, 42,  6, 11, 63, 13, 61,\n",
      "        33, 65, 65,  7, 38, 66, 42,  5, 34, 45, 25, 48, 34, 12,  1, 14,  4,  5,\n",
      "        10,  8, 59, 31, 34, 26, 60, 14, 20,  6])\n",
      "Label: tensor([ 3, 66, 30, 53, 55, 54, 36, 65, 38, 49, 61, 58, 26, 59, 34, 53,  7, 55,\n",
      "        58, 62,  7, 45,  9, 56,  1, 54, 21, 50,  9, 55, 24, 63, 16, 44, 39, 43,\n",
      "        65,  9,  8,  9, 25, 36, 51, 29, 68, 11, 70,  2,  6, 27, 24, 65, 61, 61,\n",
      "        38, 39, 66, 17, 54, 39, 42, 18, 17, 70])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: tensor([54, 36,  6, 52, 48, 12, 54, 67,  6, 12, 64, 34, 29,  7, 51, 18, 57, 32,\n",
      "        45, 49, 32, 29,  3,  2, 34, 19, 43, 34, 34, 27,  5, 21, 14, 16, 34, 26,\n",
      "        10, 34, 63, 62,  7, 11,  0, 32,  1, 15, 42,  1,  2, 45,  7, 35, 69, 63,\n",
      "        55, 64, 62, 56, 34, 56, 10, 37, 51,  4])\n",
      "Label: tensor([65, 43, 59,  2, 50, 18, 56, 62, 46, 70, 12, 72, 18, 20, 25,  3, 36, 70,\n",
      "        57, 61, 11, 30, 38, 18, 49, 24, 10, 57, 41, 57, 67, 36, 13,  6, 23, 16,\n",
      "         0,  1, 17, 70, 54, 44, 60, 20, 12, 68, 68, 32, 49, 58, 34,  9, 57,  9,\n",
      "        11, 57, 20, 54, 37, 36, 61, 19, 18, 53])\n",
      "Label: tensor([63, 34, 64, 42, 50,  8, 59, 46,  2, 20, 66, 36, 52, 70, 18, 56,  1, 11,\n",
      "        20, 28, 60, 12,  6, 37, 68, 30, 18, 13, 67, 66,  3, 16,  8,  1, 54, 54,\n",
      "        26, 61,  0, 17,  3, 55, 49, 62, 39, 62, 37, 44, 57, 40,  1,  7, 16, 45,\n",
      "        37, 58, 42, 26,  4, 20, 47,  7, 61,  9])\n",
      "Label: tensor([66, 56, 15, 38, 17, 15,  2,  3, 14, 21, 35, 45,  8, 63, 14, 43, 56, 43,\n",
      "        55, 45, 36, 45, 32, 21, 25, 46, 55, 29, 60, 14, 15, 47, 42, 62,  1, 48,\n",
      "        51, 11, 44, 52, 70, 69, 63, 40,  3, 39, 10, 12, 57,  9, 62, 56, 67, 45,\n",
      "        32, 68, 70,  1, 52, 50, 20,  5, 49, 14])\n",
      "Label: tensor([ 3, 59,  7, 40, 58, 66, 10, 44, 27, 12, 57, 69, 29, 51, 64, 18, 62,  4,\n",
      "        70,  1, 40, 65, 21, 57, 20, 59, 30, 57, 40, 65,  4, 16, 39,  3, 65, 27,\n",
      "        49, 12, 59, 63, 24, 48, 16, 46, 25, 66,  8, 66, 24, 42, 21, 22,  3,  0,\n",
      "        27, 62, 12, 54, 65, 63, 49, 14, 61, 32])\n",
      "Label: tensor([55, 37, 27, 27, 54, 24, 53, 72, 30, 24, 34, 69,  6, 67, 40, 13, 26, 52,\n",
      "        12, 42, 53, 45, 70, 15, 54,  5, 16, 51, 52, 63, 55, 50, 72, 61, 57,  4,\n",
      "        70, 14, 59, 40,  6, 16, 17, 53, 46, 17, 51, 61, 47, 71, 48, 54, 63, 13,\n",
      "        29, 49, 10, 71, 31, 20, 18, 30, 70, 67])\n",
      "Label: tensor([70, 16, 37, 10, 27, 48,  9, 11, 14, 38,  7, 16, 50,  2, 37, 37, 22,  5,\n",
      "        34,  1,  3, 42, 56, 14, 70, 12,  1, 71, 14, 21, 72, 58,  4,  7, 31,  0,\n",
      "        45, 64,  9, 54, 62,  0, 63,  6, 59, 71, 64,  4, 68, 56, 49, 40, 65, 68,\n",
      "        54, 26, 27, 21, 39, 56, 46, 35, 22, 40])\n",
      "Label: tensor([22, 41, 26,  2, 54, 70, 53, 70, 42, 14, 23,  1, 61, 45, 40, 48, 42, 20,\n",
      "        16, 11, 38, 63, 24, 12,  7, 10,  0,  6, 40,  2, 52, 56, 57, 51, 53, 20,\n",
      "         1, 65,  5, 67, 22, 39, 53, 37, 54, 63, 58, 65, 54,  5,  2, 39, 30, 51,\n",
      "        56, 16, 52, 57, 16, 30, 29, 14, 59, 40])\n",
      "Label: tensor([54, 49,  2, 72, 43,  8, 55,  5, 63, 54, 44, 56, 11, 27, 35,  4, 22, 22,\n",
      "        29, 12, 45, 22, 43, 62, 62, 23, 10, 44, 65, 54, 34,  6, 16, 69, 61,  7,\n",
      "        19, 51,  2,  5, 29, 45, 52, 46,  7, 43, 27, 54, 68, 24,  4, 67, 40, 17,\n",
      "        31,  4, 59,  4, 32, 42, 35, 11, 68, 31])\n",
      "Label: tensor([22, 44,  4, 35, 21, 48, 19, 12, 67, 49, 11, 50, 64, 51, 37, 47, 11, 68,\n",
      "        17, 50, 11, 61, 62,  4,  1, 20, 38, 29, 70, 71, 55, 70, 19, 18, 44, 22,\n",
      "        18, 65, 53, 12, 72, 61, 71, 55, 29, 33, 32, 12, 15, 62,  8, 42, 67, 59,\n",
      "        34, 60, 47, 42, 12, 51, 22, 53,  1, 17])\n",
      "Label: tensor([43, 40,  9, 43,  2, 34, 23, 16, 68, 34, 45,  5, 46, 44, 49, 62,  6, 16,\n",
      "         1, 25, 50, 14, 10, 59,  6, 44,  8, 33, 14, 42, 58, 40, 21, 10, 47, 64,\n",
      "        62, 49, 46, 26, 29, 68, 21, 61,  2, 43, 47,  0, 31, 30, 13, 67, 24,  6,\n",
      "         3, 30, 58,  4, 17,  0, 61, 41, 47, 72])\n",
      "Label: tensor([46, 45, 72, 63, 32,  8, 16, 25, 20, 46, 26, 49, 54, 17, 72, 37,  7, 14,\n",
      "         4, 37,  0, 14, 40, 53, 55,  2, 32,  3, 22, 30, 71, 37, 67, 62, 15, 69,\n",
      "        42, 39,  4, 42, 29, 14, 58, 27, 59, 60, 69, 72, 54,  4,  1, 14, 15, 60,\n",
      "        63, 48, 53, 26, 36, 30, 71, 37, 66, 49])\n",
      "Label: tensor([56, 53, 25,  5,  6,  7, 40, 66, 24, 56, 49, 57, 48, 52, 10, 58, 23, 72,\n",
      "        13, 43, 34,  1, 12, 51, 55, 23, 38, 16, 58, 63,  8,  4, 43, 51, 70, 60,\n",
      "        56,  3, 44, 50, 20, 71, 20, 56, 47, 64, 17,  1, 17, 32, 17, 68, 50, 45,\n",
      "        46, 33, 39, 63, 54,  0, 70,  6, 34, 22])\n",
      "Label: tensor([ 9, 19, 11, 34, 72, 24, 18, 38, 72, 17, 19, 15, 16, 52,  2, 29, 72, 24,\n",
      "        11, 47, 40, 23, 22, 20, 34, 45, 11, 62, 36, 27, 42, 52, 71, 44, 61, 62,\n",
      "        61, 57, 60, 17, 57, 31, 68, 62, 15, 39,  4, 55, 13, 38, 23, 50,  2, 46,\n",
      "        69, 32, 45, 56, 26, 35, 10, 62, 66, 70])\n",
      "Label: tensor([48, 47, 64, 53, 38, 48,  1, 64, 72, 51, 44, 51, 21, 54, 38, 24, 63,  0,\n",
      "        49, 23, 58, 43, 55, 26, 54, 48, 34,  7, 27, 35, 49, 35,  0, 49, 38, 29,\n",
      "        35, 65,  5, 47, 58, 17, 72, 30, 20, 69, 57, 19, 70, 68, 61, 55, 46, 23,\n",
      "        39, 29,  7, 52, 71, 37, 11, 27, 17, 12])\n",
      "Label: tensor([ 7, 14, 51, 30, 55,  7,  9, 24, 12,  4, 66,  6,  5, 52,  5, 21, 24, 51,\n",
      "        14,  1, 30, 56,  5, 40, 24, 72, 44,  1, 20, 70,  6, 56, 37, 39, 34, 64,\n",
      "        20, 71, 51, 24, 63,  8, 64, 29, 66, 71,  3, 11, 25, 34, 35, 20,  8, 62,\n",
      "         8, 65, 57, 17, 30, 67, 25, 32, 61, 69])\n",
      "Label: tensor([ 5,  6, 16, 68, 71, 60, 36, 35, 40, 62, 67, 57, 58, 72, 72, 20, 57, 20,\n",
      "        29, 57, 35, 59, 63, 52, 42,  0, 44, 36, 58, 45,  4, 43, 65, 61, 26, 44,\n",
      "        20, 45, 22, 24, 35, 51,  0, 51,  8, 23, 47, 30, 10, 49, 13, 38, 46, 18,\n",
      "         7, 68, 41, 35,  0, 42,  1, 33, 72, 13])\n",
      "Label: tensor([ 0,  5, 22,  9, 38, 68, 27, 15, 17, 47,  3, 23,  0, 58, 29, 61, 24, 18,\n",
      "        42, 59, 65,  7, 54,  4, 58, 44,  3, 70, 58,  6, 62,  1, 52, 22, 55, 37,\n",
      "        70, 57, 21, 39, 56, 63, 29, 13, 54, 21, 44, 62, 67, 24,  5, 19, 29, 67,\n",
      "        16, 48,  2, 24, 12,  9, 56,  3, 40, 46])\n",
      "Label: tensor([ 3, 20, 13, 42, 54, 54, 49, 20, 55, 44,  5, 51,  8, 26, 30, 62, 66, 20,\n",
      "         2, 16, 59, 29, 46, 14, 46, 67, 51, 13, 49, 65, 59, 52, 18, 71, 45, 33,\n",
      "        64, 55,  8, 30,  9, 55, 36, 66,  6, 18, 31, 71, 56, 57, 63, 37,  9, 62,\n",
      "        41, 59, 17, 68, 56, 31, 57, 39, 65, 45])\n",
      "Label: tensor([22,  1, 23, 48, 72, 25, 51,  7, 54, 45, 30, 63,  3, 46, 44, 50, 53, 27,\n",
      "        14,  2, 24, 65, 36, 65, 51, 38, 54, 11, 19,  9, 22, 58,  5, 48, 35, 57,\n",
      "        39, 36, 14, 38,  3, 63, 66,  2, 57, 24, 47, 20, 20, 15, 26, 22,  7, 18,\n",
      "        53, 56, 38, 58, 19, 23, 28, 20, 10, 22])\n",
      "Label: tensor([26, 32,  7, 47, 10, 32, 45, 22, 24, 19, 12, 18, 11, 51, 59, 46, 66, 54,\n",
      "        11, 50, 29, 53, 40, 30, 11, 62, 10,  4, 59, 29, 67, 12, 67, 61, 11, 46,\n",
      "        39, 63, 65, 18, 10,  7,  9, 17, 49, 63, 56, 39, 24, 51, 17, 63, 54, 51,\n",
      "        32, 37, 50, 34, 39, 16, 27, 22, 19, 47])\n",
      "Label: tensor([66, 19, 38, 60, 12, 67, 33, 45, 49, 57, 57, 50, 15, 71, 29, 45, 50, 14,\n",
      "         0, 26, 26, 54,  3, 46, 32, 40,  1, 10, 43, 61, 12,  3,  6, 17, 56, 45,\n",
      "        39,  6, 38, 41, 17, 67,  0, 45, 11, 24, 12,  9, 68, 32, 10, 20, 14, 14,\n",
      "        38, 27, 36, 33, 16,  7, 16, 36, 64, 50])\n",
      "Label: tensor([34, 34, 69, 49,  4, 57,  3,  1, 37, 62, 30,  2, 44, 54, 27, 13, 21, 29,\n",
      "        64, 46, 66,  8,  9, 41, 45, 21, 64, 31,  7, 51,  0, 12, 57, 19,  6, 15,\n",
      "        69, 32, 65,  0, 23, 35, 49,  3, 64, 27, 54,  5, 46,  6, 62,  8, 60,  7,\n",
      "        60, 24, 56, 17,  4,  9, 58, 61,  0, 72])\n",
      "Label: tensor([18,  0, 22, 32, 70,  2, 62, 70, 21,  1,  7, 55, 68, 16, 27, 47, 53,  8,\n",
      "        38, 16, 45, 23, 64,  6, 35,  7, 36, 25, 18, 46, 21, 39, 64, 17, 48,  4,\n",
      "        70, 15, 66, 64, 22, 63, 69, 71, 14, 14, 48, 49, 24, 25, 31, 49, 29, 47,\n",
      "         6, 51,  7, 11, 39, 66, 24, 53, 32, 54])\n",
      "Label: tensor([ 9, 38,  5, 62, 54, 48, 14, 34, 55, 44, 65, 14,  8, 60, 45, 65, 50,  8,\n",
      "        45, 39, 39, 58, 22, 54, 31, 52, 37, 15, 69, 62, 70, 59,  7, 27, 30, 49,\n",
      "        25, 36, 47, 21, 47, 32, 58, 20, 47, 42, 36,  2, 64, 23, 43, 56, 65,  2,\n",
      "        38, 72, 60, 31, 45, 10, 12, 58, 21, 48])\n",
      "Label: tensor([49, 46,  7, 54, 30, 67, 33, 10, 42, 16, 19, 46, 30, 45, 59, 59, 39,  2,\n",
      "        24,  9, 38, 32, 32, 10,  5, 61, 63, 17, 51, 51, 32, 62, 49, 61, 30, 26,\n",
      "        42, 63, 18, 58, 49, 40, 49, 63, 34,  7, 38, 45, 60, 40, 61,  8, 22, 20,\n",
      "        49, 17, 54, 31, 24, 17, 30, 19, 26, 60])\n",
      "Label: tensor([44, 49,  2, 53, 27, 24, 62,  4, 68, 15, 46, 16, 68, 36, 67, 38, 54, 14,\n",
      "        46,  9, 30, 52, 60, 65, 57, 10, 27, 53, 49, 15,  6, 52, 24, 45, 68, 69,\n",
      "        10, 43, 43, 34, 57, 58, 64, 10, 69, 38, 51,  7, 52, 57, 47, 32, 65,  3,\n",
      "         4, 21, 42, 24, 21,  1, 44, 46, 54, 19])\n",
      "Label: tensor([61,  2, 19, 69,  7, 23, 51, 57, 22, 46, 27, 22, 24,  2, 54, 70, 72, 26,\n",
      "        42, 40,  6, 20, 30, 51, 10,  2, 20, 35, 44, 69, 16, 12, 54,  5, 63, 65,\n",
      "        36, 67, 38, 40, 54, 10, 48,  9, 48, 19, 30, 66,  2, 63, 29, 13,  7, 39,\n",
      "        29, 62, 38, 63, 39, 23, 36, 14, 42,  4])\n",
      "Label: tensor([26, 37, 32,  7, 32, 11, 15,  1, 16,  3,  2, 42, 31, 47, 12, 42, 64, 69,\n",
      "        48, 35, 50,  5, 10, 37, 19, 39, 36, 72, 47, 19,  7, 23, 20, 17, 66, 26,\n",
      "        52, 45, 38, 63,  4, 13, 40, 65, 63, 25, 65, 46, 42, 54, 70, 37, 57, 51,\n",
      "        60,  2, 45, 20,  6,  4, 62, 35, 38, 23])\n",
      "Label: tensor([ 5, 30,  6,  3,  1, 61, 15,  7, 53, 36, 67, 42, 44, 53, 26, 57, 10, 11,\n",
      "        29,  4, 43, 48, 62, 27, 15,  9, 18, 61, 43, 30, 42, 37, 11, 46, 62, 12,\n",
      "        51, 66, 11,  4, 55, 34, 18, 56, 72, 43, 39, 10, 21,  5, 69, 40,  6, 25,\n",
      "        23,  6,  8, 54, 63, 14, 35,  5, 58, 42])\n",
      "Label: tensor([67,  8, 59, 27, 68, 16,  8, 32, 64, 10, 63, 56, 26,  5, 55, 64,  0, 14,\n",
      "        37,  6, 21, 24, 57, 64, 52, 42, 59, 13, 47,  6, 26,  8, 55, 26, 57, 15,\n",
      "        34, 29, 59, 16, 15, 54, 70, 61, 61, 66, 49,  4, 15, 43, 50, 22, 49, 17,\n",
      "        23, 21, 63, 35, 49,  8,  6, 61, 54, 36])\n",
      "Label: tensor([48, 38, 65,  6, 52, 16, 58, 49, 30, 49, 65, 13, 23, 57, 50, 47, 11, 55,\n",
      "         8, 11, 38, 11, 14, 24, 35,  2, 20, 13,  3, 12, 22, 23, 32, 52, 20, 35,\n",
      "        69, 48, 45,  8, 49, 20, 66, 42, 31, 57, 19, 62, 11, 24, 62,  8, 39,  8,\n",
      "        53,  2, 52, 57, 62,  7, 14,  5, 47, 61])\n",
      "Label: tensor([63, 49, 14, 36, 28, 58, 52, 22, 56, 70,  4, 36, 32, 62, 50, 17, 17,  1,\n",
      "        51, 61, 61, 47, 34, 57, 45, 54, 49, 52, 72, 44, 18, 42,  1, 27,  3, 63,\n",
      "        50, 67, 22, 45,  0,  3, 55, 57, 38,  9, 55, 40, 57, 10, 57, 49, 17, 18,\n",
      "        68, 42, 34, 55, 47, 71,  2, 19, 66, 39])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: tensor([43, 68, 52, 67, 49,  6, 36, 13, 62, 53, 68, 14, 72, 47, 69, 64, 47,  5,\n",
      "        11, 20, 46, 34, 16, 37,  9,  7,  9, 55,  7, 12, 61, 24, 14, 11, 49,  2,\n",
      "        12, 64,  9,  8, 64, 32, 48, 69, 52, 56,  5, 23, 49, 27, 56, 38, 66, 26,\n",
      "         4, 66, 72,  0, 70, 19,  0, 15, 58, 62])\n",
      "Label: tensor([39, 59, 11, 34, 35, 57, 37, 71,  3, 18, 39, 38, 69, 59, 34, 71, 55, 22,\n",
      "        29, 55, 10, 71, 68, 62, 24, 37, 55, 31,  3, 66, 21, 22, 37, 51, 14, 42,\n",
      "        39, 19, 47, 12, 70, 29,  4, 66, 46, 13, 10, 27, 22, 45,  7, 30, 39,  1,\n",
      "        68, 57, 64, 29, 16, 51, 20, 22, 43, 15])\n",
      "Label: tensor([35, 17, 23, 25,  9,  0, 54, 45, 56, 60, 72, 17,  2, 25, 59, 50, 70, 34,\n",
      "        18,  7, 30, 34, 35, 44, 57, 45, 18, 71, 46, 18, 59, 60, 37,  3, 18, 26,\n",
      "        30, 51, 20, 37,  2, 18, 55, 31, 29,  9, 47, 71, 49, 46, 14, 20,  7, 20,\n",
      "        34, 46, 31, 29,  5, 69, 30, 70,  7, 37])\n",
      "Label: tensor([12, 36, 31, 54,  3,  0, 40, 55, 17, 54, 61,  5, 26, 71,  6,  2, 38, 23,\n",
      "        14, 24, 64, 33, 46, 70, 32, 28, 51, 61, 22, 58, 60, 59, 48, 13, 32, 52,\n",
      "        38, 57,  3,  7, 34, 51,  0, 48, 68, 63, 70, 24, 38, 47,  9,  6, 52, 58,\n",
      "        11, 71, 47, 17,  1, 64, 45, 69, 61,  4])\n",
      "Label: tensor([24, 11, 33, 30, 42, 42, 42, 65, 19, 70, 58, 36, 25, 25, 49, 45, 64,  5,\n",
      "        51, 52, 27,  1, 23, 14,  5, 44, 15, 20, 40,  0, 67, 44, 70, 64, 23, 12,\n",
      "        53,  5, 63, 48, 12, 60, 17, 46, 51, 48, 45, 50, 56, 12, 14, 19, 58, 34,\n",
      "        65, 47, 55, 57, 37, 38, 15, 31, 23, 49])\n",
      "Label: tensor([56, 63, 59, 18, 30, 33, 20,  8, 72, 38, 17, 14, 49, 39, 66, 63, 67,  2,\n",
      "        42,  5, 16, 64, 48, 65, 60, 71,  2, 33,  7, 67,  5, 17, 47, 26, 35, 15,\n",
      "        25, 52,  8, 72, 49, 42, 32, 37, 43, 58, 56,  3, 21, 49, 69, 67, 16, 31,\n",
      "        26, 62, 57, 60, 48, 44, 22, 24, 56, 13])\n",
      "Label: tensor([19, 42, 27, 51,  8, 70, 63, 59, 59, 32, 57,  8, 70, 52, 24,  2,  2, 34,\n",
      "        64, 55, 48, 30, 67, 26, 42, 56, 59, 44, 48, 12,  1, 45, 11, 50, 16, 51,\n",
      "        36, 39, 62, 62, 44,  7, 41, 40, 45,  6, 26, 23, 10,  5, 66, 16, 64, 51,\n",
      "         6, 35, 36, 50, 38, 18, 24, 23, 34, 57])\n",
      "Label: tensor([36, 12, 14, 56, 21, 26,  8, 35, 49, 21, 14, 26, 70, 68, 65, 33, 57, 15,\n",
      "        16, 60, 29,  5, 20, 11, 42, 27, 40, 20, 51, 50, 24, 68, 23, 50, 32, 68,\n",
      "         7, 45, 35, 12,  0, 40, 65, 70, 27, 72,  2, 14, 49, 46, 57, 50, 60, 62,\n",
      "        35, 27,  6, 18, 27, 10, 40, 24, 33, 32])\n",
      "Label: tensor([46,  2, 21,  7, 54, 27, 43, 25, 12, 63, 44, 37,  4, 67, 66, 51,  1, 49,\n",
      "        68, 51, 38, 55, 11, 52, 70, 45, 51,  7,  5, 31, 52, 53, 42, 61, 10, 66,\n",
      "        63, 24, 43, 20, 65, 63,  8, 57, 47, 23, 10, 51, 62,  6, 35, 63, 49, 49,\n",
      "        30,  9, 67, 57, 17,  6, 22,  8, 21, 55])\n",
      "Label: tensor([18,  5, 46, 41, 57, 68, 49, 30, 21, 36, 17, 65, 51, 59, 32, 42,  3, 48,\n",
      "        61, 42, 15, 50, 37, 19, 10,  8, 62,  9, 25, 66, 56, 41, 63, 20, 29, 65,\n",
      "        68, 17,  2, 68, 17,  4, 64, 71, 67, 42, 58, 52, 50, 58, 66, 46, 66, 53,\n",
      "        42, 30, 51, 65, 66, 66, 47,  6, 22, 12])\n",
      "Label: tensor([21, 16, 39, 10, 26, 27, 30, 50, 16, 39, 72, 72, 61, 61, 21, 64, 15, 29,\n",
      "        62, 72, 72, 46, 24,  6,  9, 33, 25, 71, 55, 27, 25, 10, 67,  0, 58,  4,\n",
      "        21, 16, 70, 55,  6, 40, 10, 39,  1,  8, 69, 69, 55, 63, 18, 39,  9, 50,\n",
      "        33, 52, 16, 63, 15, 11, 44,  4, 21,  0])\n",
      "Label: tensor([31, 15, 64, 31, 23,  0, 34, 67, 12, 35, 60, 53, 44, 57, 22, 42, 72, 57,\n",
      "        23,  0, 49, 71, 44, 12, 15, 62, 57, 45, 31, 37, 12, 20, 40,  5, 65, 50,\n",
      "        50,  0,  3,  9, 69, 70, 17, 19,  9, 24, 11, 23, 45, 48, 27,  4, 51, 44,\n",
      "        22, 25,  7, 54, 62, 59,  2, 46, 31, 64])\n",
      "Label: tensor([16, 21, 13, 23, 60, 37, 42, 11, 13,  6,  0, 17, 61, 40, 32, 34, 61, 24,\n",
      "        51, 29, 25, 15,  6, 68, 48, 37, 33, 37, 38, 20, 27, 64, 56, 71, 19, 19,\n",
      "        23,  1, 10, 68, 59, 34, 26, 63, 54, 72,  5, 42, 40,  3, 62, 36, 29,  8,\n",
      "        27, 52, 54, 13, 12, 19, 52,  8, 71, 50])\n",
      "Label: tensor([17, 47, 44,  5, 38, 53, 45,  3, 38, 10, 66,  5, 15,  1, 52, 16, 34,  2,\n",
      "         5, 36, 55, 71, 15, 59,  3, 55, 25, 26, 46, 50,  7,  0, 26, 56, 69, 58,\n",
      "        24, 18, 70,  8, 46, 23, 53, 69, 61, 20,  8, 16, 59, 69, 34, 12,  2, 53,\n",
      "        67,  0, 54, 42,  4, 27, 50, 36, 67,  8])\n",
      "Label: tensor([70,  0, 12, 21, 24, 36,  5, 36, 39,  9, 66, 26, 32, 20, 38,  4, 43, 70,\n",
      "         1, 23, 37, 45, 49, 36,  9, 10, 33, 14, 25, 52, 61, 15, 15, 38, 64, 25,\n",
      "        33, 70, 10, 44, 27, 56, 43, 30, 57, 16, 13,  0, 29,  5, 32, 51, 50, 27,\n",
      "        12, 70, 36,  4, 36, 64, 21, 60, 24, 58])\n",
      "Label: tensor([37, 20, 58, 45, 62, 68, 42, 25, 33, 23, 44, 49, 57, 57, 16, 22, 32, 21,\n",
      "        12,  4, 59, 50, 56, 47, 19, 59, 24, 26,  8, 11, 30, 30, 70, 58, 68, 14,\n",
      "        62, 63, 13, 25,  6,  8, 22,  8, 24,  8, 10, 62, 53, 33,  7,  8, 33, 41,\n",
      "        57, 38, 47, 11,  9, 38, 38, 24, 49, 71])\n",
      "Label: tensor([ 0,  7, 27, 39, 10, 26, 19, 55, 19, 42, 63,  4,  4, 72, 62, 38, 61, 43,\n",
      "        63, 44, 68, 53, 65, 37, 64,  9, 32, 13, 68, 21, 14, 17, 27,  9, 51, 64,\n",
      "        36, 43, 12, 10,  1, 68, 15, 70, 29, 45, 63, 27, 39])\n"
     ]
    }
   ],
   "source": [
    "for image, label in trainloader:\n",
    "    print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Everything else from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv11): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv12): Conv2d(20, 20, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (batch1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv21): Conv2d(20, 40, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv22): Conv2d(40, 40, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (batch2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv31): Conv2d(40, 80, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv32): Conv2d(80, 80, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (batch3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1280, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=73, bias=True)\n",
      "  (dropconv): Dropout(p=0.2, inplace=False)\n",
      "  (dropfc): Dropout(p=0.4, inplace=False)\n",
      "  (soft): LogSoftmax()\n",
      ")\n",
      "NLLLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.005\n",
      "    weight_decay: 0\n",
      ")\n",
      "========================================BEGIN TRAINING=======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zacan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:105: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  [Epoch: 1, Batch: 10]   Train loss: 0.524  |  Test loss: 4.325  |  Test accuracy: 0.006  |\n",
      "|  [Epoch: 1, Batch: 20]   Train loss: 0.508  |  Test loss: 3.821  |  Test accuracy: 0.051  |\n",
      "|  [Epoch: 1, Batch: 30]   Train loss: 0.488  |  Test loss: 3.918  |  Test accuracy: 0.031  |\n",
      "|  [Epoch: 1, Batch: 40]   Train loss: 0.478  |  Test loss: 3.652  |  Test accuracy: 0.061  |\n",
      "|  [Epoch: 1, Batch: 50]   Train loss: 0.455  |  Test loss: 3.129  |  Test accuracy: 0.152  |\n",
      "|  [Epoch: 1, Batch: 60]   Train loss: 0.446  |  Test loss: 3.631  |  Test accuracy: 0.068  |\n",
      "|  [Epoch: 1, Batch: 70]   Train loss: 0.443  |  Test loss: 2.984  |  Test accuracy: 0.148  |\n",
      "|  [Epoch: 1, Batch: 80]   Train loss: 0.434  |  Test loss: 3.333  |  Test accuracy: 0.115  |\n",
      "|  [Epoch: 2, Batch: 10]   Train loss: 0.518  |  Test loss: 2.490  |  Test accuracy: 0.280  |\n",
      "|  [Epoch: 2, Batch: 20]   Train loss: 0.383  |  Test loss: 3.189  |  Test accuracy: 0.123  |\n",
      "|  [Epoch: 2, Batch: 30]   Train loss: 0.389  |  Test loss: 2.480  |  Test accuracy: 0.273  |\n",
      "|  [Epoch: 2, Batch: 40]   Train loss: 0.366  |  Test loss: 2.153  |  Test accuracy: 0.368  |\n",
      "|  [Epoch: 2, Batch: 50]   Train loss: 0.350  |  Test loss: 3.384  |  Test accuracy: 0.179  |\n",
      "|  [Epoch: 2, Batch: 60]   Train loss: 0.355  |  Test loss: 2.556  |  Test accuracy: 0.250  |\n",
      "|  [Epoch: 2, Batch: 70]   Train loss: 0.353  |  Test loss: 2.157  |  Test accuracy: 0.352  |\n",
      "|  [Epoch: 2, Batch: 80]   Train loss: 0.335  |  Test loss: 1.819  |  Test accuracy: 0.442  |\n",
      "|  [Epoch: 3, Batch: 10]   Train loss: 0.417  |  Test loss: 2.628  |  Test accuracy: 0.228  |\n",
      "|  [Epoch: 3, Batch: 20]   Train loss: 0.314  |  Test loss: 1.768  |  Test accuracy: 0.453  |\n",
      "|  [Epoch: 3, Batch: 30]   Train loss: 0.334  |  Test loss: 1.927  |  Test accuracy: 0.389  |\n",
      "|  [Epoch: 3, Batch: 40]   Train loss: 0.327  |  Test loss: 1.888  |  Test accuracy: 0.412  |\n",
      "|  [Epoch: 3, Batch: 50]   Train loss: 0.311  |  Test loss: 1.771  |  Test accuracy: 0.440  |\n",
      "|  [Epoch: 3, Batch: 60]   Train loss: 0.303  |  Test loss: 1.461  |  Test accuracy: 0.534  |\n",
      "|  [Epoch: 3, Batch: 70]   Train loss: 0.315  |  Test loss: 1.449  |  Test accuracy: 0.523  |\n",
      "|  [Epoch: 3, Batch: 80]   Train loss: 0.302  |  Test loss: 1.903  |  Test accuracy: 0.398  |\n",
      "|  [Epoch: 4, Batch: 10]   Train loss: 0.379  |  Test loss: 1.616  |  Test accuracy: 0.453  |\n",
      "|  [Epoch: 4, Batch: 20]   Train loss: 0.287  |  Test loss: 1.285  |  Test accuracy: 0.626  |\n",
      "|  [Epoch: 4, Batch: 30]   Train loss: 0.289  |  Test loss: 1.952  |  Test accuracy: 0.410  |\n",
      "|  [Epoch: 4, Batch: 40]   Train loss: 0.285  |  Test loss: 1.306  |  Test accuracy: 0.554  |\n",
      "|  [Epoch: 4, Batch: 50]   Train loss: 0.287  |  Test loss: 1.293  |  Test accuracy: 0.574  |\n",
      "|  [Epoch: 4, Batch: 60]   Train loss: 0.274  |  Test loss: 1.851  |  Test accuracy: 0.456  |\n",
      "|  [Epoch: 4, Batch: 70]   Train loss: 0.280  |  Test loss: 1.834  |  Test accuracy: 0.416  |\n",
      "|  [Epoch: 4, Batch: 80]   Train loss: 0.273  |  Test loss: 1.505  |  Test accuracy: 0.512  |\n",
      "|  [Epoch: 5, Batch: 10]   Train loss: 0.370  |  Test loss: 1.101  |  Test accuracy: 0.632  |\n",
      "|  [Epoch: 5, Batch: 20]   Train loss: 0.264  |  Test loss: 1.127  |  Test accuracy: 0.626  |\n",
      "|  [Epoch: 5, Batch: 30]   Train loss: 0.266  |  Test loss: 1.099  |  Test accuracy: 0.623  |\n",
      "|  [Epoch: 5, Batch: 40]   Train loss: 0.277  |  Test loss: 1.225  |  Test accuracy: 0.592  |\n",
      "|  [Epoch: 5, Batch: 50]   Train loss: 0.268  |  Test loss: 1.846  |  Test accuracy: 0.418  |\n",
      "|  [Epoch: 5, Batch: 60]   Train loss: 0.257  |  Test loss: 1.107  |  Test accuracy: 0.648  |\n",
      "|  [Epoch: 5, Batch: 70]   Train loss: 0.247  |  Test loss: 1.105  |  Test accuracy: 0.614  |\n",
      "|  [Epoch: 5, Batch: 80]   Train loss: 0.238  |  Test loss: 1.643  |  Test accuracy: 0.448  |\n",
      "|  [Epoch: 6, Batch: 10]   Train loss: 0.322  |  Test loss: 1.701  |  Test accuracy: 0.460  |\n",
      "|  [Epoch: 6, Batch: 20]   Train loss: 0.238  |  Test loss: 0.822  |  Test accuracy: 0.690  |\n",
      "|  [Epoch: 6, Batch: 30]   Train loss: 0.231  |  Test loss: 0.945  |  Test accuracy: 0.686  |\n",
      "|  [Epoch: 6, Batch: 40]   Train loss: 0.242  |  Test loss: 0.903  |  Test accuracy: 0.693  |\n",
      "|  [Epoch: 6, Batch: 50]   Train loss: 0.250  |  Test loss: 0.872  |  Test accuracy: 0.690  |\n",
      "|  [Epoch: 6, Batch: 60]   Train loss: 0.254  |  Test loss: 0.932  |  Test accuracy: 0.693  |\n",
      "|  [Epoch: 6, Batch: 70]   Train loss: 0.239  |  Test loss: 0.810  |  Test accuracy: 0.705  |\n",
      "|  [Epoch: 6, Batch: 80]   Train loss: 0.257  |  Test loss: 0.732  |  Test accuracy: 0.743  |\n",
      "|  [Epoch: 7, Batch: 10]   Train loss: 0.336  |  Test loss: 0.736  |  Test accuracy: 0.729  |\n",
      "|  [Epoch: 7, Batch: 20]   Train loss: 0.237  |  Test loss: 0.784  |  Test accuracy: 0.733  |\n",
      "|  [Epoch: 7, Batch: 30]   Train loss: 0.217  |  Test loss: 0.805  |  Test accuracy: 0.729  |\n",
      "|  [Epoch: 7, Batch: 40]   Train loss: 0.217  |  Test loss: 0.865  |  Test accuracy: 0.693  |\n",
      "|  [Epoch: 7, Batch: 50]   Train loss: 0.234  |  Test loss: 0.743  |  Test accuracy: 0.725  |\n",
      "|  [Epoch: 7, Batch: 60]   Train loss: 0.217  |  Test loss: 1.050  |  Test accuracy: 0.631  |\n",
      "|  [Epoch: 7, Batch: 70]   Train loss: 0.216  |  Test loss: 0.772  |  Test accuracy: 0.733  |\n",
      "|  [Epoch: 7, Batch: 80]   Train loss: 0.231  |  Test loss: 0.634  |  Test accuracy: 0.765  |\n",
      "|  [Epoch: 8, Batch: 10]   Train loss: 0.277  |  Test loss: 1.390  |  Test accuracy: 0.565  |\n",
      "|  [Epoch: 8, Batch: 20]   Train loss: 0.221  |  Test loss: 0.740  |  Test accuracy: 0.748  |\n",
      "|  [Epoch: 8, Batch: 30]   Train loss: 0.207  |  Test loss: 0.563  |  Test accuracy: 0.784  |\n",
      "|  [Epoch: 8, Batch: 40]   Train loss: 0.207  |  Test loss: 0.508  |  Test accuracy: 0.798  |\n",
      "|  [Epoch: 8, Batch: 50]   Train loss: 0.218  |  Test loss: 0.631  |  Test accuracy: 0.745  |\n",
      "|  [Epoch: 8, Batch: 60]   Train loss: 0.226  |  Test loss: 0.626  |  Test accuracy: 0.769  |\n",
      "|  [Epoch: 8, Batch: 70]   Train loss: 0.240  |  Test loss: 1.371  |  Test accuracy: 0.556  |\n",
      "|  [Epoch: 8, Batch: 80]   Train loss: 0.231  |  Test loss: 1.665  |  Test accuracy: 0.512  |\n",
      "|  [Epoch: 9, Batch: 10]   Train loss: 0.293  |  Test loss: 0.688  |  Test accuracy: 0.755  |\n",
      "|  [Epoch: 9, Batch: 20]   Train loss: 0.212  |  Test loss: 1.449  |  Test accuracy: 0.556  |\n",
      "|  [Epoch: 9, Batch: 30]   Train loss: 0.246  |  Test loss: 0.611  |  Test accuracy: 0.772  |\n",
      "|  [Epoch: 9, Batch: 40]   Train loss: 0.219  |  Test loss: 0.566  |  Test accuracy: 0.794  |\n",
      "|  [Epoch: 9, Batch: 50]   Train loss: 0.227  |  Test loss: 0.739  |  Test accuracy: 0.728  |\n",
      "|  [Epoch: 9, Batch: 60]   Train loss: 0.214  |  Test loss: 0.640  |  Test accuracy: 0.755  |\n",
      "|  [Epoch: 9, Batch: 70]   Train loss: 0.216  |  Test loss: 0.607  |  Test accuracy: 0.764  |\n",
      "|  [Epoch: 9, Batch: 80]   Train loss: 0.225  |  Test loss: 0.619  |  Test accuracy: 0.762  |\n",
      "|  [Epoch: 10, Batch: 10]   Train loss: 0.283  |  Test loss: 0.707  |  Test accuracy: 0.733  |\n",
      "|  [Epoch: 10, Batch: 20]   Train loss: 0.215  |  Test loss: 0.556  |  Test accuracy: 0.778  |\n",
      "|  [Epoch: 10, Batch: 30]   Train loss: 0.189  |  Test loss: 1.072  |  Test accuracy: 0.633  |\n",
      "|  [Epoch: 10, Batch: 40]   Train loss: 0.211  |  Test loss: 0.852  |  Test accuracy: 0.696  |\n",
      "|  [Epoch: 10, Batch: 50]   Train loss: 0.201  |  Test loss: 0.558  |  Test accuracy: 0.784  |\n",
      "|  [Epoch: 10, Batch: 60]   Train loss: 0.215  |  Test loss: 0.476  |  Test accuracy: 0.804  |\n",
      "|  [Epoch: 10, Batch: 70]   Train loss: 0.218  |  Test loss: 0.959  |  Test accuracy: 0.676  |\n",
      "|  [Epoch: 10, Batch: 80]   Train loss: 0.214  |  Test loss: 0.564  |  Test accuracy: 0.785  |\n",
      "|  [Epoch: 11, Batch: 10]   Train loss: 0.301  |  Test loss: 0.554  |  Test accuracy: 0.780  |\n",
      "|  [Epoch: 11, Batch: 20]   Train loss: 0.208  |  Test loss: 0.534  |  Test accuracy: 0.797  |\n",
      "|  [Epoch: 11, Batch: 30]   Train loss: 0.217  |  Test loss: 0.411  |  Test accuracy: 0.819  |\n",
      "|  [Epoch: 11, Batch: 40]   Train loss: 0.205  |  Test loss: 0.458  |  Test accuracy: 0.811  |\n",
      "|  [Epoch: 11, Batch: 50]   Train loss: 0.208  |  Test loss: 0.484  |  Test accuracy: 0.816  |\n",
      "|  [Epoch: 11, Batch: 60]   Train loss: 0.202  |  Test loss: 0.507  |  Test accuracy: 0.790  |\n",
      "|  [Epoch: 11, Batch: 70]   Train loss: 0.199  |  Test loss: 0.741  |  Test accuracy: 0.724  |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  [Epoch: 11, Batch: 80]   Train loss: 0.212  |  Test loss: 0.582  |  Test accuracy: 0.774  |\n",
      "|  [Epoch: 12, Batch: 10]   Train loss: 0.262  |  Test loss: 0.698  |  Test accuracy: 0.728  |\n",
      "|  [Epoch: 12, Batch: 20]   Train loss: 0.206  |  Test loss: 0.702  |  Test accuracy: 0.759  |\n",
      "|  [Epoch: 12, Batch: 30]   Train loss: 0.213  |  Test loss: 1.225  |  Test accuracy: 0.616  |\n",
      "|  [Epoch: 12, Batch: 40]   Train loss: 0.191  |  Test loss: 0.425  |  Test accuracy: 0.805  |\n",
      "|  [Epoch: 12, Batch: 50]   Train loss: 0.217  |  Test loss: 0.363  |  Test accuracy: 0.830  |\n",
      "|  [Epoch: 12, Batch: 60]   Train loss: 0.198  |  Test loss: 0.541  |  Test accuracy: 0.767  |\n",
      "|  [Epoch: 12, Batch: 70]   Train loss: 0.203  |  Test loss: 0.553  |  Test accuracy: 0.771  |\n",
      "|  [Epoch: 12, Batch: 80]   Train loss: 0.205  |  Test loss: 0.557  |  Test accuracy: 0.777  |\n",
      "|  [Epoch: 13, Batch: 10]   Train loss: 0.266  |  Test loss: 0.483  |  Test accuracy: 0.795  |\n",
      "|  [Epoch: 13, Batch: 20]   Train loss: 0.199  |  Test loss: 0.409  |  Test accuracy: 0.822  |\n",
      "|  [Epoch: 13, Batch: 30]   Train loss: 0.196  |  Test loss: 0.705  |  Test accuracy: 0.731  |\n",
      "|  [Epoch: 13, Batch: 40]   Train loss: 0.198  |  Test loss: 0.584  |  Test accuracy: 0.772  |\n",
      "|  [Epoch: 13, Batch: 50]   Train loss: 0.196  |  Test loss: 0.524  |  Test accuracy: 0.791  |\n",
      "|  [Epoch: 13, Batch: 60]   Train loss: 0.221  |  Test loss: 0.445  |  Test accuracy: 0.826  |\n",
      "|  [Epoch: 13, Batch: 70]   Train loss: 0.198  |  Test loss: 0.404  |  Test accuracy: 0.821  |\n",
      "|  [Epoch: 13, Batch: 80]   Train loss: 0.211  |  Test loss: 0.447  |  Test accuracy: 0.810  |\n",
      "|  [Epoch: 14, Batch: 10]   Train loss: 0.237  |  Test loss: 0.345  |  Test accuracy: 0.833  |\n",
      "|  [Epoch: 14, Batch: 20]   Train loss: 0.195  |  Test loss: 0.395  |  Test accuracy: 0.827  |\n",
      "|  [Epoch: 14, Batch: 30]   Train loss: 0.208  |  Test loss: 0.517  |  Test accuracy: 0.799  |\n",
      "|  [Epoch: 14, Batch: 40]   Train loss: 0.183  |  Test loss: 0.321  |  Test accuracy: 0.841  |\n",
      "|  [Epoch: 14, Batch: 50]   Train loss: 0.195  |  Test loss: 0.375  |  Test accuracy: 0.836  |\n",
      "|  [Epoch: 14, Batch: 60]   Train loss: 0.203  |  Test loss: 0.368  |  Test accuracy: 0.835  |\n",
      "|  [Epoch: 14, Batch: 70]   Train loss: 0.188  |  Test loss: 0.387  |  Test accuracy: 0.830  |\n",
      "|  [Epoch: 14, Batch: 80]   Train loss: 0.206  |  Test loss: 0.329  |  Test accuracy: 0.851  |\n",
      "|  [Epoch: 15, Batch: 10]   Train loss: 0.258  |  Test loss: 0.331  |  Test accuracy: 0.841  |\n",
      "|  [Epoch: 15, Batch: 20]   Train loss: 0.185  |  Test loss: 0.741  |  Test accuracy: 0.738  |\n",
      "|  [Epoch: 15, Batch: 30]   Train loss: 0.190  |  Test loss: 0.467  |  Test accuracy: 0.798  |\n",
      "|  [Epoch: 15, Batch: 40]   Train loss: 0.208  |  Test loss: 0.650  |  Test accuracy: 0.751  |\n",
      "|  [Epoch: 15, Batch: 50]   Train loss: 0.191  |  Test loss: 0.482  |  Test accuracy: 0.800  |\n",
      "|  [Epoch: 15, Batch: 60]   Train loss: 0.195  |  Test loss: 0.461  |  Test accuracy: 0.807  |\n",
      "|  [Epoch: 15, Batch: 70]   Train loss: 0.201  |  Test loss: 0.361  |  Test accuracy: 0.829  |\n",
      "|  [Epoch: 15, Batch: 80]   Train loss: 0.198  |  Test loss: 0.498  |  Test accuracy: 0.810  |\n",
      "|  [Epoch: 16, Batch: 10]   Train loss: 0.258  |  Test loss: 0.415  |  Test accuracy: 0.815  |\n",
      "|  [Epoch: 16, Batch: 20]   Train loss: 0.189  |  Test loss: 0.370  |  Test accuracy: 0.836  |\n",
      "|  [Epoch: 16, Batch: 30]   Train loss: 0.202  |  Test loss: 0.374  |  Test accuracy: 0.835  |\n",
      "|  [Epoch: 16, Batch: 40]   Train loss: 0.184  |  Test loss: 0.349  |  Test accuracy: 0.845  |\n",
      "|  [Epoch: 16, Batch: 50]   Train loss: 0.190  |  Test loss: 0.393  |  Test accuracy: 0.825  |\n",
      "|  [Epoch: 16, Batch: 60]   Train loss: 0.205  |  Test loss: 0.409  |  Test accuracy: 0.812  |\n",
      "|  [Epoch: 16, Batch: 70]   Train loss: 0.194  |  Test loss: 0.351  |  Test accuracy: 0.843  |\n",
      "|  [Epoch: 16, Batch: 80]   Train loss: 0.211  |  Test loss: 0.317  |  Test accuracy: 0.854  |\n",
      "|  [Epoch: 17, Batch: 10]   Train loss: 0.240  |  Test loss: 0.625  |  Test accuracy: 0.761  |\n",
      "|  [Epoch: 17, Batch: 20]   Train loss: 0.187  |  Test loss: 0.345  |  Test accuracy: 0.845  |\n",
      "|  [Epoch: 17, Batch: 30]   Train loss: 0.193  |  Test loss: 0.330  |  Test accuracy: 0.844  |\n",
      "|  [Epoch: 17, Batch: 40]   Train loss: 0.191  |  Test loss: 0.409  |  Test accuracy: 0.824  |\n",
      "|  [Epoch: 17, Batch: 50]   Train loss: 0.195  |  Test loss: 0.338  |  Test accuracy: 0.844  |\n",
      "|  [Epoch: 17, Batch: 60]   Train loss: 0.186  |  Test loss: 0.530  |  Test accuracy: 0.795  |\n",
      "|  [Epoch: 17, Batch: 70]   Train loss: 0.199  |  Test loss: 0.415  |  Test accuracy: 0.812  |\n",
      "|  [Epoch: 17, Batch: 80]   Train loss: 0.183  |  Test loss: 0.436  |  Test accuracy: 0.814  |\n",
      "|  [Epoch: 18, Batch: 10]   Train loss: 0.254  |  Test loss: 0.361  |  Test accuracy: 0.830  |\n",
      "|  [Epoch: 18, Batch: 20]   Train loss: 0.209  |  Test loss: 0.390  |  Test accuracy: 0.834  |\n",
      "|  [Epoch: 18, Batch: 30]   Train loss: 0.194  |  Test loss: 0.327  |  Test accuracy: 0.848  |\n",
      "|  [Epoch: 18, Batch: 40]   Train loss: 0.183  |  Test loss: 0.287  |  Test accuracy: 0.855  |\n",
      "|  [Epoch: 18, Batch: 50]   Train loss: 0.183  |  Test loss: 0.353  |  Test accuracy: 0.842  |\n",
      "|  [Epoch: 18, Batch: 60]   Train loss: 0.202  |  Test loss: 0.282  |  Test accuracy: 0.858  |\n",
      "|  [Epoch: 18, Batch: 70]   Train loss: 0.176  |  Test loss: 0.371  |  Test accuracy: 0.834  |\n",
      "|  [Epoch: 18, Batch: 80]   Train loss: 0.175  |  Test loss: 0.387  |  Test accuracy: 0.841  |\n",
      "|  [Epoch: 19, Batch: 10]   Train loss: 0.228  |  Test loss: 0.287  |  Test accuracy: 0.854  |\n",
      "|  [Epoch: 19, Batch: 20]   Train loss: 0.193  |  Test loss: 0.504  |  Test accuracy: 0.806  |\n",
      "|  [Epoch: 19, Batch: 30]   Train loss: 0.176  |  Test loss: 0.418  |  Test accuracy: 0.826  |\n",
      "|  [Epoch: 19, Batch: 40]   Train loss: 0.196  |  Test loss: 0.320  |  Test accuracy: 0.838  |\n",
      "|  [Epoch: 19, Batch: 50]   Train loss: 0.172  |  Test loss: 0.437  |  Test accuracy: 0.824  |\n",
      "|  [Epoch: 19, Batch: 60]   Train loss: 0.203  |  Test loss: 0.323  |  Test accuracy: 0.854  |\n",
      "|  [Epoch: 19, Batch: 70]   Train loss: 0.195  |  Test loss: 0.356  |  Test accuracy: 0.845  |\n",
      "|  [Epoch: 19, Batch: 80]   Train loss: 0.193  |  Test loss: 0.406  |  Test accuracy: 0.821  |\n",
      "|  [Epoch: 20, Batch: 10]   Train loss: 0.238  |  Test loss: 0.297  |  Test accuracy: 0.864  |\n",
      "|  [Epoch: 20, Batch: 20]   Train loss: 0.166  |  Test loss: 0.324  |  Test accuracy: 0.843  |\n",
      "|  [Epoch: 20, Batch: 30]   Train loss: 0.164  |  Test loss: 0.415  |  Test accuracy: 0.821  |\n",
      "|  [Epoch: 20, Batch: 40]   Train loss: 0.196  |  Test loss: 0.296  |  Test accuracy: 0.868  |\n",
      "|  [Epoch: 20, Batch: 50]   Train loss: 0.184  |  Test loss: 0.356  |  Test accuracy: 0.849  |\n",
      "|  [Epoch: 20, Batch: 60]   Train loss: 0.204  |  Test loss: 0.313  |  Test accuracy: 0.860  |\n",
      "|  [Epoch: 20, Batch: 70]   Train loss: 0.189  |  Test loss: 0.378  |  Test accuracy: 0.827  |\n",
      "|  [Epoch: 20, Batch: 80]   Train loss: 0.191  |  Test loss: 0.364  |  Test accuracy: 0.836  |\n",
      "|  [Epoch: 21, Batch: 10]   Train loss: 0.243  |  Test loss: 0.492  |  Test accuracy: 0.803  |\n",
      "|  [Epoch: 21, Batch: 20]   Train loss: 0.180  |  Test loss: 0.270  |  Test accuracy: 0.864  |\n",
      "|  [Epoch: 21, Batch: 30]   Train loss: 0.199  |  Test loss: 0.303  |  Test accuracy: 0.863  |\n",
      "|  [Epoch: 21, Batch: 40]   Train loss: 0.195  |  Test loss: 0.281  |  Test accuracy: 0.863  |\n",
      "|  [Epoch: 21, Batch: 50]   Train loss: 0.183  |  Test loss: 0.263  |  Test accuracy: 0.864  |\n",
      "|  [Epoch: 21, Batch: 60]   Train loss: 0.188  |  Test loss: 0.439  |  Test accuracy: 0.812  |\n",
      "|  [Epoch: 21, Batch: 70]   Train loss: 0.180  |  Test loss: 0.266  |  Test accuracy: 0.865  |\n",
      "|  [Epoch: 21, Batch: 80]   Train loss: 0.192  |  Test loss: 0.228  |  Test accuracy: 0.877  |\n",
      "|  [Epoch: 22, Batch: 10]   Train loss: 0.232  |  Test loss: 0.369  |  Test accuracy: 0.835  |\n",
      "|  [Epoch: 22, Batch: 20]   Train loss: 0.169  |  Test loss: 0.255  |  Test accuracy: 0.867  |\n",
      "|  [Epoch: 22, Batch: 30]   Train loss: 0.181  |  Test loss: 0.293  |  Test accuracy: 0.864  |\n",
      "|  [Epoch: 22, Batch: 40]   Train loss: 0.172  |  Test loss: 0.244  |  Test accuracy: 0.872  |\n",
      "|  [Epoch: 22, Batch: 50]   Train loss: 0.176  |  Test loss: 0.229  |  Test accuracy: 0.880  |\n",
      "|  [Epoch: 22, Batch: 60]   Train loss: 0.177  |  Test loss: 0.252  |  Test accuracy: 0.874  |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  [Epoch: 22, Batch: 70]   Train loss: 0.179  |  Test loss: 0.281  |  Test accuracy: 0.865  |\n",
      "|  [Epoch: 22, Batch: 80]   Train loss: 0.187  |  Test loss: 0.259  |  Test accuracy: 0.872  |\n",
      "|  [Epoch: 23, Batch: 10]   Train loss: 0.238  |  Test loss: 0.259  |  Test accuracy: 0.868  |\n",
      "|  [Epoch: 23, Batch: 20]   Train loss: 0.173  |  Test loss: 0.292  |  Test accuracy: 0.857  |\n",
      "|  [Epoch: 23, Batch: 30]   Train loss: 0.183  |  Test loss: 0.284  |  Test accuracy: 0.859  |\n",
      "|  [Epoch: 23, Batch: 40]   Train loss: 0.182  |  Test loss: 0.285  |  Test accuracy: 0.868  |\n",
      "|  [Epoch: 23, Batch: 50]   Train loss: 0.181  |  Test loss: 0.288  |  Test accuracy: 0.858  |\n",
      "|  [Epoch: 23, Batch: 60]   Train loss: 0.188  |  Test loss: 0.469  |  Test accuracy: 0.811  |\n",
      "|  [Epoch: 23, Batch: 70]   Train loss: 0.188  |  Test loss: 0.362  |  Test accuracy: 0.839  |\n",
      "|  [Epoch: 23, Batch: 80]   Train loss: 0.190  |  Test loss: 0.247  |  Test accuracy: 0.864  |\n",
      "|  [Epoch: 24, Batch: 10]   Train loss: 0.236  |  Test loss: 0.330  |  Test accuracy: 0.849  |\n",
      "|  [Epoch: 24, Batch: 20]   Train loss: 0.187  |  Test loss: 0.446  |  Test accuracy: 0.811  |\n",
      "|  [Epoch: 24, Batch: 30]   Train loss: 0.195  |  Test loss: 0.334  |  Test accuracy: 0.835  |\n",
      "|  [Epoch: 24, Batch: 40]   Train loss: 0.180  |  Test loss: 0.425  |  Test accuracy: 0.826  |\n",
      "|  [Epoch: 24, Batch: 50]   Train loss: 0.187  |  Test loss: 0.323  |  Test accuracy: 0.846  |\n",
      "|  [Epoch: 24, Batch: 60]   Train loss: 0.183  |  Test loss: 0.510  |  Test accuracy: 0.786  |\n",
      "|  [Epoch: 24, Batch: 70]   Train loss: 0.199  |  Test loss: 0.244  |  Test accuracy: 0.872  |\n",
      "|  [Epoch: 24, Batch: 80]   Train loss: 0.179  |  Test loss: 0.276  |  Test accuracy: 0.863  |\n",
      "|  [Epoch: 25, Batch: 10]   Train loss: 0.247  |  Test loss: 0.344  |  Test accuracy: 0.855  |\n",
      "|  [Epoch: 25, Batch: 20]   Train loss: 0.178  |  Test loss: 0.316  |  Test accuracy: 0.856  |\n",
      "|  [Epoch: 25, Batch: 30]   Train loss: 0.195  |  Test loss: 0.334  |  Test accuracy: 0.849  |\n",
      "|  [Epoch: 25, Batch: 40]   Train loss: 0.168  |  Test loss: 0.351  |  Test accuracy: 0.841  |\n",
      "|  [Epoch: 25, Batch: 50]   Train loss: 0.170  |  Test loss: 0.321  |  Test accuracy: 0.855  |\n",
      "|  [Epoch: 25, Batch: 60]   Train loss: 0.166  |  Test loss: 0.279  |  Test accuracy: 0.854  |\n",
      "|  [Epoch: 25, Batch: 70]   Train loss: 0.181  |  Test loss: 0.559  |  Test accuracy: 0.801  |\n",
      "|  [Epoch: 25, Batch: 80]   Train loss: 0.197  |  Test loss: 0.338  |  Test accuracy: 0.850  |\n",
      "|  [Epoch: 26, Batch: 10]   Train loss: 0.253  |  Test loss: 0.291  |  Test accuracy: 0.854  |\n",
      "|  [Epoch: 26, Batch: 20]   Train loss: 0.166  |  Test loss: 0.341  |  Test accuracy: 0.845  |\n",
      "|  [Epoch: 26, Batch: 30]   Train loss: 0.180  |  Test loss: 0.243  |  Test accuracy: 0.866  |\n",
      "|  [Epoch: 26, Batch: 40]   Train loss: 0.178  |  Test loss: 0.214  |  Test accuracy: 0.882  |\n",
      "|  [Epoch: 26, Batch: 50]   Train loss: 0.183  |  Test loss: 0.266  |  Test accuracy: 0.870  |\n",
      "|  [Epoch: 26, Batch: 60]   Train loss: 0.170  |  Test loss: 0.236  |  Test accuracy: 0.877  |\n",
      "|  [Epoch: 26, Batch: 70]   Train loss: 0.178  |  Test loss: 0.255  |  Test accuracy: 0.861  |\n",
      "|  [Epoch: 26, Batch: 80]   Train loss: 0.180  |  Test loss: 0.434  |  Test accuracy: 0.816  |\n",
      "|  [Epoch: 27, Batch: 10]   Train loss: 0.247  |  Test loss: 0.255  |  Test accuracy: 0.867  |\n",
      "|  [Epoch: 27, Batch: 20]   Train loss: 0.175  |  Test loss: 0.318  |  Test accuracy: 0.852  |\n",
      "|  [Epoch: 27, Batch: 30]   Train loss: 0.188  |  Test loss: 0.357  |  Test accuracy: 0.851  |\n",
      "|  [Epoch: 27, Batch: 40]   Train loss: 0.180  |  Test loss: 0.345  |  Test accuracy: 0.849  |\n",
      "|  [Epoch: 27, Batch: 50]   Train loss: 0.181  |  Test loss: 0.390  |  Test accuracy: 0.842  |\n",
      "|  [Epoch: 27, Batch: 60]   Train loss: 0.178  |  Test loss: 0.316  |  Test accuracy: 0.849  |\n",
      "|  [Epoch: 27, Batch: 70]   Train loss: 0.179  |  Test loss: 0.373  |  Test accuracy: 0.832  |\n",
      "|  [Epoch: 27, Batch: 80]   Train loss: 0.189  |  Test loss: 0.329  |  Test accuracy: 0.857  |\n",
      "|  [Epoch: 28, Batch: 10]   Train loss: 0.235  |  Test loss: 0.299  |  Test accuracy: 0.855  |\n",
      "|  [Epoch: 28, Batch: 20]   Train loss: 0.181  |  Test loss: 0.330  |  Test accuracy: 0.848  |\n",
      "|  [Epoch: 28, Batch: 30]   Train loss: 0.188  |  Test loss: 0.323  |  Test accuracy: 0.851  |\n",
      "|  [Epoch: 28, Batch: 40]   Train loss: 0.189  |  Test loss: 0.322  |  Test accuracy: 0.852  |\n",
      "|  [Epoch: 28, Batch: 50]   Train loss: 0.176  |  Test loss: 0.472  |  Test accuracy: 0.812  |\n",
      "|  [Epoch: 28, Batch: 60]   Train loss: 0.192  |  Test loss: 0.344  |  Test accuracy: 0.842  |\n",
      "|  [Epoch: 28, Batch: 70]   Train loss: 0.185  |  Test loss: 0.259  |  Test accuracy: 0.865  |\n",
      "|  [Epoch: 28, Batch: 80]   Train loss: 0.185  |  Test loss: 0.221  |  Test accuracy: 0.878  |\n",
      "|  [Epoch: 29, Batch: 10]   Train loss: 0.246  |  Test loss: 0.250  |  Test accuracy: 0.869  |\n",
      "|  [Epoch: 29, Batch: 20]   Train loss: 0.172  |  Test loss: 0.225  |  Test accuracy: 0.873  |\n",
      "|  [Epoch: 29, Batch: 30]   Train loss: 0.191  |  Test loss: 0.273  |  Test accuracy: 0.863  |\n",
      "|  [Epoch: 29, Batch: 40]   Train loss: 0.178  |  Test loss: 0.251  |  Test accuracy: 0.864  |\n",
      "|  [Epoch: 29, Batch: 50]   Train loss: 0.165  |  Test loss: 0.279  |  Test accuracy: 0.865  |\n",
      "|  [Epoch: 29, Batch: 60]   Train loss: 0.184  |  Test loss: 0.343  |  Test accuracy: 0.840  |\n",
      "|  [Epoch: 29, Batch: 70]   Train loss: 0.170  |  Test loss: 0.242  |  Test accuracy: 0.871  |\n",
      "|  [Epoch: 29, Batch: 80]   Train loss: 0.172  |  Test loss: 0.306  |  Test accuracy: 0.858  |\n",
      "|  [Epoch: 30, Batch: 10]   Train loss: 0.222  |  Test loss: 0.233  |  Test accuracy: 0.876  |\n",
      "|  [Epoch: 30, Batch: 20]   Train loss: 0.169  |  Test loss: 0.370  |  Test accuracy: 0.829  |\n",
      "|  [Epoch: 30, Batch: 30]   Train loss: 0.187  |  Test loss: 0.276  |  Test accuracy: 0.852  |\n",
      "|  [Epoch: 30, Batch: 40]   Train loss: 0.186  |  Test loss: 0.284  |  Test accuracy: 0.861  |\n",
      "|  [Epoch: 30, Batch: 50]   Train loss: 0.174  |  Test loss: 0.252  |  Test accuracy: 0.872  |\n",
      "|  [Epoch: 30, Batch: 60]   Train loss: 0.189  |  Test loss: 0.216  |  Test accuracy: 0.876  |\n",
      "|  [Epoch: 30, Batch: 70]   Train loss: 0.176  |  Test loss: 0.295  |  Test accuracy: 0.858  |\n",
      "|  [Epoch: 30, Batch: 80]   Train loss: 0.182  |  Test loss: 0.300  |  Test accuracy: 0.856  |\n",
      "|  [Epoch: 31, Batch: 10]   Train loss: 0.230  |  Test loss: 0.251  |  Test accuracy: 0.871  |\n",
      "|  [Epoch: 31, Batch: 20]   Train loss: 0.177  |  Test loss: 0.201  |  Test accuracy: 0.882  |\n",
      "|  [Epoch: 31, Batch: 30]   Train loss: 0.174  |  Test loss: 0.241  |  Test accuracy: 0.876  |\n",
      "|  [Epoch: 31, Batch: 40]   Train loss: 0.169  |  Test loss: 0.264  |  Test accuracy: 0.865  |\n",
      "|  [Epoch: 31, Batch: 50]   Train loss: 0.182  |  Test loss: 0.237  |  Test accuracy: 0.875  |\n",
      "|  [Epoch: 31, Batch: 60]   Train loss: 0.165  |  Test loss: 0.302  |  Test accuracy: 0.865  |\n",
      "|  [Epoch: 31, Batch: 70]   Train loss: 0.173  |  Test loss: 0.275  |  Test accuracy: 0.853  |\n",
      "|  [Epoch: 31, Batch: 80]   Train loss: 0.149  |  Test loss: 0.256  |  Test accuracy: 0.866  |\n",
      "|  [Epoch: 32, Batch: 10]   Train loss: 0.228  |  Test loss: 0.240  |  Test accuracy: 0.876  |\n",
      "|  [Epoch: 32, Batch: 20]   Train loss: 0.171  |  Test loss: 0.276  |  Test accuracy: 0.867  |\n",
      "|  [Epoch: 32, Batch: 30]   Train loss: 0.174  |  Test loss: 0.221  |  Test accuracy: 0.880  |\n",
      "|  [Epoch: 32, Batch: 40]   Train loss: 0.165  |  Test loss: 0.234  |  Test accuracy: 0.882  |\n",
      "|  [Epoch: 32, Batch: 50]   Train loss: 0.167  |  Test loss: 0.300  |  Test accuracy: 0.873  |\n",
      "|  [Epoch: 32, Batch: 60]   Train loss: 0.171  |  Test loss: 0.283  |  Test accuracy: 0.868  |\n",
      "|  [Epoch: 32, Batch: 70]   Train loss: 0.174  |  Test loss: 0.315  |  Test accuracy: 0.860  |\n",
      "|  [Epoch: 32, Batch: 80]   Train loss: 0.176  |  Test loss: 0.212  |  Test accuracy: 0.882  |\n",
      "|  [Epoch: 33, Batch: 10]   Train loss: 0.227  |  Test loss: 0.299  |  Test accuracy: 0.863  |\n",
      "|  [Epoch: 33, Batch: 20]   Train loss: 0.174  |  Test loss: 0.255  |  Test accuracy: 0.870  |\n",
      "|  [Epoch: 33, Batch: 30]   Train loss: 0.173  |  Test loss: 0.940  |  Test accuracy: 0.711  |\n",
      "|  [Epoch: 33, Batch: 40]   Train loss: 0.182  |  Test loss: 0.264  |  Test accuracy: 0.867  |\n",
      "|  [Epoch: 33, Batch: 50]   Train loss: 0.190  |  Test loss: 0.278  |  Test accuracy: 0.868  |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  [Epoch: 33, Batch: 60]   Train loss: 0.184  |  Test loss: 0.317  |  Test accuracy: 0.852  |\n",
      "|  [Epoch: 33, Batch: 70]   Train loss: 0.164  |  Test loss: 0.232  |  Test accuracy: 0.870  |\n",
      "|  [Epoch: 33, Batch: 80]   Train loss: 0.160  |  Test loss: 0.233  |  Test accuracy: 0.874  |\n",
      "|  [Epoch: 34, Batch: 10]   Train loss: 0.219  |  Test loss: 0.256  |  Test accuracy: 0.872  |\n",
      "|  [Epoch: 34, Batch: 20]   Train loss: 0.175  |  Test loss: 0.243  |  Test accuracy: 0.875  |\n",
      "|  [Epoch: 34, Batch: 30]   Train loss: 0.182  |  Test loss: 0.258  |  Test accuracy: 0.870  |\n",
      "|  [Epoch: 34, Batch: 40]   Train loss: 0.160  |  Test loss: 0.261  |  Test accuracy: 0.868  |\n",
      "|  [Epoch: 34, Batch: 50]   Train loss: 0.164  |  Test loss: 0.242  |  Test accuracy: 0.871  |\n",
      "|  [Epoch: 34, Batch: 60]   Train loss: 0.170  |  Test loss: 0.255  |  Test accuracy: 0.875  |\n",
      "|  [Epoch: 34, Batch: 70]   Train loss: 0.185  |  Test loss: 0.245  |  Test accuracy: 0.879  |\n",
      "|  [Epoch: 34, Batch: 80]   Train loss: 0.180  |  Test loss: 0.250  |  Test accuracy: 0.871  |\n",
      "|  [Epoch: 35, Batch: 10]   Train loss: 0.221  |  Test loss: 0.263  |  Test accuracy: 0.870  |\n",
      "|  [Epoch: 35, Batch: 20]   Train loss: 0.184  |  Test loss: 0.234  |  Test accuracy: 0.873  |\n",
      "|  [Epoch: 35, Batch: 30]   Train loss: 0.163  |  Test loss: 0.195  |  Test accuracy: 0.887  |\n",
      "|  [Epoch: 35, Batch: 40]   Train loss: 0.172  |  Test loss: 0.285  |  Test accuracy: 0.858  |\n",
      "|  [Epoch: 35, Batch: 50]   Train loss: 0.178  |  Test loss: 0.339  |  Test accuracy: 0.855  |\n",
      "|  [Epoch: 35, Batch: 60]   Train loss: 0.182  |  Test loss: 0.298  |  Test accuracy: 0.860  |\n",
      "|  [Epoch: 35, Batch: 70]   Train loss: 0.182  |  Test loss: 0.410  |  Test accuracy: 0.834  |\n",
      "|  [Epoch: 35, Batch: 80]   Train loss: 0.176  |  Test loss: 0.255  |  Test accuracy: 0.868  |\n",
      "|  [Epoch: 36, Batch: 10]   Train loss: 0.233  |  Test loss: 0.303  |  Test accuracy: 0.867  |\n",
      "|  [Epoch: 36, Batch: 20]   Train loss: 0.188  |  Test loss: 0.379  |  Test accuracy: 0.851  |\n",
      "|  [Epoch: 36, Batch: 30]   Train loss: 0.190  |  Test loss: 0.356  |  Test accuracy: 0.856  |\n",
      "|  [Epoch: 36, Batch: 40]   Train loss: 0.198  |  Test loss: 0.330  |  Test accuracy: 0.840  |\n",
      "|  [Epoch: 36, Batch: 50]   Train loss: 0.179  |  Test loss: 0.439  |  Test accuracy: 0.827  |\n",
      "|  [Epoch: 36, Batch: 60]   Train loss: 0.179  |  Test loss: 0.286  |  Test accuracy: 0.875  |\n",
      "|  [Epoch: 36, Batch: 70]   Train loss: 0.190  |  Test loss: 0.299  |  Test accuracy: 0.858  |\n",
      "|  [Epoch: 36, Batch: 80]   Train loss: 0.163  |  Test loss: 0.336  |  Test accuracy: 0.855  |\n",
      "|  [Epoch: 37, Batch: 10]   Train loss: 0.217  |  Test loss: 0.288  |  Test accuracy: 0.864  |\n",
      "|  [Epoch: 37, Batch: 20]   Train loss: 0.171  |  Test loss: 0.304  |  Test accuracy: 0.858  |\n",
      "|  [Epoch: 37, Batch: 30]   Train loss: 0.173  |  Test loss: 0.242  |  Test accuracy: 0.865  |\n",
      "|  [Epoch: 37, Batch: 40]   Train loss: 0.182  |  Test loss: 0.255  |  Test accuracy: 0.866  |\n",
      "|  [Epoch: 37, Batch: 50]   Train loss: 0.155  |  Test loss: 0.224  |  Test accuracy: 0.880  |\n",
      "|  [Epoch: 37, Batch: 60]   Train loss: 0.171  |  Test loss: 0.265  |  Test accuracy: 0.861  |\n",
      "|  [Epoch: 37, Batch: 70]   Train loss: 0.159  |  Test loss: 0.192  |  Test accuracy: 0.883  |\n",
      "|  [Epoch: 37, Batch: 80]   Train loss: 0.174  |  Test loss: 0.244  |  Test accuracy: 0.866  |\n",
      "|  [Epoch: 38, Batch: 10]   Train loss: 0.219  |  Test loss: 0.228  |  Test accuracy: 0.873  |\n",
      "|  [Epoch: 38, Batch: 20]   Train loss: 0.162  |  Test loss: 0.241  |  Test accuracy: 0.873  |\n",
      "|  [Epoch: 38, Batch: 30]   Train loss: 0.173  |  Test loss: 0.202  |  Test accuracy: 0.884  |\n",
      "|  [Epoch: 38, Batch: 40]   Train loss: 0.173  |  Test loss: 0.245  |  Test accuracy: 0.872  |\n",
      "|  [Epoch: 38, Batch: 50]   Train loss: 0.168  |  Test loss: 0.275  |  Test accuracy: 0.874  |\n",
      "|  [Epoch: 38, Batch: 60]   Train loss: 0.186  |  Test loss: 0.272  |  Test accuracy: 0.864  |\n",
      "|  [Epoch: 38, Batch: 70]   Train loss: 0.189  |  Test loss: 0.287  |  Test accuracy: 0.860  |\n",
      "|  [Epoch: 38, Batch: 80]   Train loss: 0.169  |  Test loss: 0.502  |  Test accuracy: 0.803  |\n",
      "|  [Epoch: 39, Batch: 10]   Train loss: 0.243  |  Test loss: 0.239  |  Test accuracy: 0.878  |\n",
      "|  [Epoch: 39, Batch: 20]   Train loss: 0.174  |  Test loss: 0.230  |  Test accuracy: 0.877  |\n",
      "|  [Epoch: 39, Batch: 30]   Train loss: 0.177  |  Test loss: 0.239  |  Test accuracy: 0.868  |\n",
      "|  [Epoch: 39, Batch: 40]   Train loss: 0.164  |  Test loss: 0.212  |  Test accuracy: 0.879  |\n",
      "|  [Epoch: 39, Batch: 50]   Train loss: 0.171  |  Test loss: 0.236  |  Test accuracy: 0.875  |\n",
      "|  [Epoch: 39, Batch: 60]   Train loss: 0.161  |  Test loss: 0.599  |  Test accuracy: 0.797  |\n",
      "|  [Epoch: 39, Batch: 70]   Train loss: 0.176  |  Test loss: 0.224  |  Test accuracy: 0.873  |\n",
      "|  [Epoch: 39, Batch: 80]   Train loss: 0.181  |  Test loss: 0.287  |  Test accuracy: 0.864  |\n",
      "|  [Epoch: 40, Batch: 10]   Train loss: 0.213  |  Test loss: 0.354  |  Test accuracy: 0.857  |\n",
      "|  [Epoch: 40, Batch: 20]   Train loss: 0.168  |  Test loss: 0.308  |  Test accuracy: 0.867  |\n",
      "|  [Epoch: 40, Batch: 30]   Train loss: 0.164  |  Test loss: 0.274  |  Test accuracy: 0.870  |\n",
      "|  [Epoch: 40, Batch: 40]   Train loss: 0.172  |  Test loss: 0.218  |  Test accuracy: 0.876  |\n",
      "|  [Epoch: 40, Batch: 50]   Train loss: 0.163  |  Test loss: 0.232  |  Test accuracy: 0.878  |\n",
      "|  [Epoch: 40, Batch: 60]   Train loss: 0.170  |  Test loss: 0.360  |  Test accuracy: 0.839  |\n",
      "|  [Epoch: 40, Batch: 70]   Train loss: 0.168  |  Test loss: 0.306  |  Test accuracy: 0.863  |\n",
      "|  [Epoch: 40, Batch: 80]   Train loss: 0.174  |  Test loss: 0.194  |  Test accuracy: 0.884  |\n",
      "=========================================END TRAINING========================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv11 = nn.Conv2d(in_channels=num_output_channels, out_channels=20, kernel_size=5, padding=2)\n",
    "        self.conv12 = nn.Conv2d(20, 20, 5, padding=2)\n",
    "        self.batch1 = nn.BatchNorm2d(20)\n",
    "        self.pool1  = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.conv21 = nn.Conv2d(20, 40, 5, padding=2)\n",
    "        self.conv22 = nn.Conv2d(40, 40, 5, padding=2)\n",
    "        self.batch2 = nn.BatchNorm2d(40)\n",
    "        self.pool2  = nn.MaxPool2d(2,2)\n",
    "        self.conv31 = nn.Conv2d(40, 80, 5, padding=2)\n",
    "        self.conv32 = nn.Conv2d(80, 80, 5, padding=2)\n",
    "        self.batch3 = nn.BatchNorm2d(80)\n",
    "        self.pool3  = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        # Transitioning from Conv ===> Linear\n",
    "        # 16 is the number of output channels in the previous conv layer.\n",
    "        \n",
    "        self.fc1 = nn.Linear(80 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, len(classes))\n",
    "        self.dropconv = nn.Dropout(0.2)\n",
    "        self.dropfc = nn.Dropout(0.4)\n",
    "        self.soft = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv11(x))\n",
    "        x = F.relu(self.conv12(x))\n",
    "        x = self.batch1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv21(x))\n",
    "        x = F.relu(self.conv22(x))\n",
    "        x = self.batch2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv31(x))\n",
    "        x = F.relu(self.conv32(x))\n",
    "        x = self.batch3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(-1, 80 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.dropfc(x)\n",
    "        x = self.soft(x)\n",
    "        return x\n",
    "\n",
    "# init the class \n",
    "model = Net()\n",
    "print(model)\n",
    "model.eval()\n",
    "inp = torch.randn((1,1,32,32))\n",
    "out = model(inp)\n",
    "\n",
    "#model.load_state_dict(torch.load('./models/custom_label_1002.pt'))\n",
    "model.train()\n",
    "import torch.optim as optim\n",
    "\n",
    "# set parameters\n",
    "learning_rate = 0.005\n",
    "momentum = 0.9\n",
    "\n",
    "def loss_optim():\n",
    "    # Loss function: \n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    # criterion = nn.MSELoss()\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    # Optimizer:\n",
    "    #optimizer = torch.optim.Adamax(model.parameters(), learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "    optimizer = optim.Adam(model.parameters(), learning_rate, amsgrad = True)\n",
    "    #optimizer = optim.SGD(model.parameters(), learning_rate, momentum)\n",
    "    \n",
    "    return criterion, optimizer\n",
    "\n",
    "criterion, optimizer = loss_optim()\n",
    "print(criterion)\n",
    "print(optimizer)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def train_network():\n",
    "    \n",
    "    # Choose parameters\n",
    "    num_epoch = 40\n",
    "    mini_batch = 10 # previously batch_size\n",
    "    train_losses, test_losses = [], []\n",
    "    running_loss = 0\n",
    "    for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            # convert to float bc softmax doesn't work with long\n",
    "            inputs = torch.tensor(inputs, dtype=torch.float)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # print statistics\n",
    "            if i % mini_batch == mini_batch-1:    # print every # of mini-batches\n",
    "                test_loss = 0\n",
    "                accuracy = 0\n",
    "                # begin evaluation of validation loss\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for inputs_test, labels_test in testloader:\n",
    "                        if labels_test.size()[0] == batch_size:\n",
    "                            ps = model.forward(inputs_test)\n",
    "                            batch_loss = criterion(ps, labels_test)\n",
    "                            test_loss += batch_loss.item()\n",
    "                            top_p, top_class = ps.topk(1, dim=1)\n",
    "                            equals = top_class == labels_test.view(*top_class.shape)\n",
    "                            accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "                train_losses.append(running_loss/len(trainloader))\n",
    "                test_losses.append(test_loss/len(testloader))    \n",
    "                print(f\"|  [Epoch: {epoch + 1}, Batch: {i + 1}]   \"\n",
    "                      f\"Train loss: {running_loss/len(trainloader):.3f}  |  \"\n",
    "                      f\"Test loss: {test_loss/len(testloader):.3f}  |  \"\n",
    "                      f\"Test accuracy: {accuracy/len(testloader):.3f}  |\")\n",
    "                running_loss = 0\n",
    "                model.train()\n",
    "            \n",
    "    \n",
    "    PATH = './models/pictex_100.pt'\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    return train_losses, test_losses, PATH\n",
    "\n",
    "def visualize_train(train_losses, test_losses):\n",
    "     plt.plot(train_losses, label='Training loss')\n",
    "     plt.plot(test_losses, label='Test/Validation loss')\n",
    "     plt.legend(frameon=False)\n",
    "     plt.show()\n",
    "\n",
    "print(\"========================================BEGIN TRAINING=======================================\")\n",
    "train_losses, test_losses, PATH = train_network()\n",
    "print(\"=========================================END TRAINING========================================\")\n",
    "visualize_train(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
